{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52ca91fe",
   "metadata": {},
   "source": [
    "### Importamos las dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "89ae0fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323302d4",
   "metadata": {},
   "source": [
    "### Variables de entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d85b2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIzaSyAxoIcKAhmJDO1D7XQR8s4iry5yoOL37zI\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"API_KEY\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a057ecce",
   "metadata": {},
   "source": [
    "### Subir un Documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e6721387",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def upload_pdf(url: str):        \n",
    "    try:\n",
    "        loader = PyPDFLoader(url)\n",
    "        loader = loader.lazy_load()\n",
    "\n",
    "        text = \"\"\n",
    "\n",
    "        for page in loader: \n",
    "            text += page.page_content + \"\\n\"\n",
    "\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return []\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197416b6",
   "metadata": {},
   "source": [
    "### Text Splitter para separar todo el contenido de mi documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "db664495",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def text_splitter(text): \n",
    "\n",
    "    text_splitter = CharacterTextSplitter(\n",
    "    chunk_size = 7000,\n",
    "    chunk_overlap = 100,\n",
    "    separator=\"\\n\"\n",
    ")\n",
    "    texts = text_splitter.create_documents([text])\n",
    "    print(texts)\n",
    "    return texts\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf35666",
   "metadata": {},
   "source": [
    "### Defino el modelo que utilizaré"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "90bc142a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = OllamaEmbeddings(\n",
    "    model = \"nomic-embed-text\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193725a0",
   "metadata": {},
   "source": [
    "### Creo mi base de datos vectorial donde se guardará mi embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "db426117",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector_store(name_collection: str): \n",
    "    \n",
    "    vector_store = Chroma(\n",
    "    collection_name= name_collection,\n",
    "    embedding_function=embedding,\n",
    "    persist_directory=\"./vectorstore\"\n",
    ")    \n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7da8de",
   "metadata": {},
   "source": [
    "### Creo el retrieval que devolverá la información en una busqueda de similitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4f407b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieval(input_user: str): \n",
    "    vector_store = get_vector_store(\"langchain\")\n",
    "    docs = vector_store.similarity_search(input_user)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad17eac4",
   "metadata": {},
   "source": [
    "### Creamos el propt system para el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0b084557",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "    Eres un asistente encargado de responder preguntas sobre Arquitectura de software y solo debes contestar si el contexto no está vacio.\n",
    "    En caso de que no cuentes con la información solicitada responde \"La pregunta excede mi conocimiento\" y si te preguntan algo fuera del contexto principal responde \"No estoy programado para eso\".\n",
    "    Utiliza siempre el contexto proporcionado para responder.\n",
    "    contexto = {contexto}\n",
    "    pregunta del usuario: {input_user}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf8720a",
   "metadata": {},
   "source": [
    "### Creamos la función de respuesta que nos comunicará con nuestro agende de IA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3d64f263",
   "metadata": {},
   "outputs": [],
   "source": [
    "def response(input_user: str, contexto: str):\n",
    "    llm = ChatGoogleGenerativeAI(\n",
    "    api_key=api_key,\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    temperature= 0.5\n",
    ")\n",
    "\n",
    "    for chunk in llm.stream(prompt.format(contexto=contexto, input_user=input_user)):\n",
    "        yield chunk.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4180cea",
   "metadata": {},
   "source": [
    "### Utilizamos las funciones para cargar el doc, aplicarle el text_splitter y guardar esos datos como embedding en la base de datos vectorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d98d533d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={}, page_content='Introducción  a  RAG   La  generación  aumentada  por  recuperación  (RAG)  es  el  proceso  de  optimización  de  la  salida  \\nde\\n \\nun\\n \\nmodelo\\n \\nde\\n \\nlenguaje\\n \\nde\\n \\ngran\\n \\ntamaño,\\n \\nde\\n \\nmodo\\n \\nque\\n \\nhaga\\n \\nreferencia\\n \\na\\n \\nuna\\n \\nbase\\n \\nde\\n \\nconocimientos\\n \\nautorizada\\n \\nfuera\\n \\nde\\n \\nlos\\n \\norígenes\\n \\nde\\n \\ndatos\\n \\nde\\n \\nentrenamiento\\n \\nantes\\n \\nde\\n \\ngenerar\\n \\nuna\\n \\nrespuesta.\\n \\nLos\\n \\nmodelos\\n \\nde\\n \\nlenguaje\\n \\nde\\n \\ngran\\n \\ntamaño\\n \\n(LLM)\\n \\nse\\n \\nentrenan\\n \\ncon\\n \\nvolúmenes\\n \\nde\\n \\ndatos\\n \\namplios\\n \\ny\\n \\nusan\\n \\nmiles\\n \\nde\\n \\nmillones\\n \\nde\\n \\nparámetros\\n \\npara\\n \\ngenerar\\n \\nresultados\\n \\noriginales\\n \\nen\\n \\ntareas\\n \\ncomo\\n \\nresponder\\n \\npreguntas,\\n \\ntraducir\\n \\nidiomas\\n \\ny\\n \\ncompletar\\n \\nfrases.\\n \\nLa\\n \\nRAG\\n \\nextiende\\n \\nlas\\n \\nya\\n \\npoderosas\\n \\ncapacidades\\n \\nde\\n \\nlos\\n \\nLLM\\n \\na\\n \\ndominios\\n \\nespecíficos\\n \\no\\n \\na\\n \\nla\\n \\nbase\\n \\nde\\n \\nconocimientos\\n \\ninterna\\n \\nde\\n \\nuna\\n \\norganización,\\n \\ntodo\\n \\nello\\n \\nsin\\n \\nla\\n \\nnecesidad\\n \\nde\\n \\nvolver\\n \\na\\n \\nentrenar\\n \\nel\\n \\nmodelo.\\n \\nSe\\n \\ntrata\\n \\nde\\n \\nun\\n \\nmétodo\\n \\nrentable\\n \\npara\\n \\nmejorar\\n \\nlos\\n \\nresultados\\n \\nde\\n \\nlos\\n \\nLLM\\n \\nde\\n \\nmodo\\n \\nque\\n \\nsigan\\n \\nsiendo\\n \\nrelevantes,\\n \\nprecisos\\n \\ny\\n \\nútiles\\n \\nen\\n \\ndiversos\\n \\ncontextos.\\n   ¿Cuáles  son  los  beneficios  de  la  generación  aumentada  por  recuperación?   La  tecnología  RAG  aporta  varios  beneficios  a  los  esfuerzos  de  la  IA  generativa  de  una  \\norganización.\\n  ●  Implementación  rentable:  El  desarrollo  de  chatbots  normalmente  comienza  con  un  \\nmodelo\\n \\nfundacional.\\n \\nLos\\n \\nmodelos\\n \\nfundacionales\\n \\n(FM)\\n \\nson\\n \\nLLM\\n \\naccesibles\\n \\npara\\n \\nAPI\\n \\nentrenados\\n \\nen\\n \\nun\\n \\namplio\\n \\nespectro\\n \\nde\\n \\ndatos\\n \\ngeneralizados\\n \\ny\\n \\nsin\\n \\netiquetar.\\n \\nLos\\n \\ncostos\\n \\ncomputacionales\\n \\ny\\n \\nfinancieros\\n \\nde\\n \\nvolver\\n \\na\\n \\nentrenar\\n \\na\\n \\nlos\\n \\nFM\\n \\npara\\n \\nobtener\\n \\ninformación\\n \\nespecífica\\n \\nde\\n \\nla\\n \\norganización\\n \\no\\n \\ndel\\n \\ndominio\\n \\nson\\n \\naltos.\\n \\nLa\\n \\nRAG\\n \\nes\\n \\nun\\n \\nenfoque\\n \\nmás\\n \\nrentable\\n \\npara\\n \\nintroducir\\n \\nnuevos\\n \\ndatos\\n \\nen\\n \\nel\\n \\nLLM.\\n \\nHace\\n \\nque\\n \\nla\\n \\ntecnología\\n \\nde\\n \\ninteligencia\\n \\nartificial\\n \\ngenerativa\\n \\n(IA\\n \\ngenerativa)\\n \\nsea\\n \\nmás\\n \\naccesible\\n \\ny\\n \\nutilizable.\\n  ●  Información  actual:  Incluso  si  los  orígenes  de  datos  de  entrenamiento  originales  para  \\nun\\n \\nLLM\\n \\nson\\n \\nadecuados\\n \\npara\\n \\nsus\\n \\nnecesidades,\\n \\nes\\n \\ndifícil\\n \\nmantener\\n \\nla\\n \\nrelevancia.\\n \\nLa\\n \\nRAG\\n \\nles\\n \\npermite\\n \\na\\n \\nlos\\n \\ndesarrolladores\\n \\nproporcionar\\n \\nlas\\n \\núltimas\\n \\ninvestigaciones,\\n \\nestadísticas\\n \\no\\n \\nnoticias\\n \\na\\n \\nlos\\n \\nmodelos\\n \\ngenerativos.\\n \\nPueden\\n \\nusar\\n \\nla\\n \\nRAG\\n \\npara\\n \\nconectar\\n \\nel\\n \\nLLM\\n \\nde\\n \\nmanera\\n \\ndirecta\\n \\na\\n \\nredes\\n \\nsociales\\n \\nen\\n \\nvivo,\\n \\nsitios\\n \\nde\\n \\nnoticias\\n \\nu\\n \\notras\\n \\nfuentes\\n \\nde\\n \\ninformación\\n \\nque\\n \\nse\\n \\nactualizan\\n \\ncon\\n \\nfrecuencia.\\n \\nEl\\n \\nLLM\\n \\npuede\\n \\nentonces\\n \\nproporcionar\\n \\nla\\n \\ninformación\\n \\nmás\\n \\nreciente\\n \\na\\n \\nlos\\n \\nusuarios.\\n  ●  Mayor  confianza  de  los  usuarios:  La  RAG  le  permite  al  LLM  presentar  información  \\nprecisa\\n \\ncon\\n \\nla\\n \\natribución\\n \\nde\\n \\nla\\n \\nfuente.\\n \\nLa\\n \\nsalida\\n \\npuede\\n \\nincluir\\n \\ncitas\\n \\no\\n \\nreferencias\\n \\na\\n \\nfuentes.\\n \\nLos\\n \\nusuarios\\n \\ntambién\\n \\npueden\\n \\nbuscar\\n \\nellos\\n \\nmismos\\n \\nlos\\n \\ndocumentos\\n \\nde\\n \\norigen\\n \\nsi\\n \\nnecesitan\\n \\nmás\\n \\naclaraciones\\n \\no\\n \\nmás\\n \\ndetalles.\\n \\nEsto\\n \\npuede\\n \\naumentar\\n \\nla\\n \\nconfianza\\n \\nen\\n \\nsu\\n \\nsolución\\n \\nde\\n \\nIA\\n \\ngenerativa.\\n●  Más  control  para  los  desarrolladores:  Con  la  RAG,  los  desarrolladores  pueden  \\nprobar\\n \\ny\\n \\nmejorar\\n \\nsus\\n \\naplicaciones\\n \\nde\\n \\nchat\\n \\nde\\n \\nmanera\\n \\nmás\\n \\neficiente.\\n \\nPueden\\n \\ncontrolar\\n \\ny\\n \\ncambiar\\n \\nlas\\n \\nfuentes\\n \\nde\\n \\ninformación\\n \\ndel\\n \\nLLM\\n \\npara\\n \\nadaptarse\\n \\na\\n \\nlos\\n \\nrequisitos\\n \\ncambiantes\\n \\no\\n \\nal\\n \\nuso\\n \\nmultifuncional.\\n \\nLos\\n \\ndesarrolladores\\n \\ntambién\\n \\npueden\\n \\nrestringir\\n \\nla\\n \\nrecuperación\\n \\nde\\n \\ninformación\\n \\nconfidencial\\n \\na\\n \\ndiferentes\\n \\nniveles\\n \\nde\\n \\nautorización\\n \\ny\\n \\ngarantizar\\n \\nque\\n \\nel\\n \\nLLM\\n \\ngenere\\n \\nlas\\n \\nrespuestas\\n \\nadecuadas.\\n \\nAdemás,\\n \\ntambién\\n \\npueden\\n \\nsolucionar\\n \\nproblemas\\n \\ny\\n \\nhacer\\n \\ncorrecciones\\n \\nsi\\n \\nel\\n \\nLLM\\n \\nhace\\n \\nreferencia\\n \\na\\n \\nfuentes\\n \\nde\\n \\ninformación\\n \\nincorrectas\\n \\npara\\n \\npreguntas\\n \\nespecíficas.\\n \\nLas\\n \\norganizaciones\\n \\npueden\\n \\nimplementar\\n \\nla\\n \\ntecnología\\n \\nde\\n \\nIA\\n \\ngenerativa\\n \\ncon\\n \\nmayor\\n \\nconfianza\\n \\npara\\n \\nuna\\n \\ngama\\n \\nmás\\n \\namplia\\n \\nde\\n \\naplicaciones.\\n  Implementación  con  Langhain   1.  Cargamos  el  documento  con  PyPDFLoader  y  lo  convertimos  en  un  iterable.  Luego  \\nrecorremos\\n \\nese\\n \\niterable\\n \\ny\\n \\nguardamos\\n \\ncada\\n \\npage\\n \\nen\\n \\nuna\\n \\nvariable.\\n \\n  from langchain_community.document_loaders  import PyPDFLoader  #  cargar  el  documento document  =  PyPDFLoader(\"LANGCHAIN.pdf\")   #  creamos  el  iterador loader  =  document.lazy_load()   text  =  \"\" #  recorremos  el  contenido  del  loader   for page  in loader:       text  +=  page.page_content  2.  Importamos  nuestro  separador  de  texto,  lo  implementamos  y  creamos  los  documentos  que  \\ncargaremos\\n \\nposteriormente\\n \\nen\\n \\nnuestra\\n \\nbase\\n \\nde\\n \\ndatos\\n \\nvectorial:\\n \\n  from langchain_text_splitters  import CharacterTextSplitter  #  creamos  nuestro  separador  de  texto text_splitter  =  CharacterTextSplitter(      chunk_size=2000,      chunk_overlap=100,      separator=\"\\\\n\" )  #  creamos  los  documentos  para  cargar  nuestra  base  de  datos  vectorial texts  =  text_splitter.create_documents([text])\\n3.  Importamos  OllamaEmbedding  desde  langchain_ollama  para  utilizar  el  modelo  de  \\nembedding\\n  from langchain_ollama  import OllamaEmbeddings  #  creamos  la  comunicacion  con  nuestro  modelo  de  embedding embedding  =  OllamaEmbeddings(      model=\"nomic-embed-text\" )     4.  Ahora,  importamos  Chroma  desde  langchain_chroma  y  creamos  nuestra  vector  store   from langchain_chroma  import Chroma   #  declaramos  la  base  de  datos  vectorial vector_store  =  Chroma(      collection_name=\"test\",      embedding_function=embedding,      persist_directory=\"./vectorstore-nomic\" )  5.  Cargamos  los  documentos  que  creamos  en  el  punto  2   #  creamos  los  embeddings  y  lo  guardamos  en  la  base  de  datos  vectorial   vector_store.add_documents(texts)  6.  En  este  punto,  ya  tenemos  documentos  en  nuestra  base  de  datos  vectorial,  por  lo  tanto,  \\nvamos\\n \\na\\n \\ncrear\\n \\nnuestro\\n \\nllm\\n \\nque\\n \\nresponderá\\n \\npreguntas\\n \\nutilizando\\n \\nla\\n \\ninformación\\n \\nque\\n \\nguardamos'), Document(metadata={}, page_content='nuestro\\n \\nllm\\n \\nque\\n \\nresponderá\\n \\npreguntas\\n \\nutilizando\\n \\nla\\n \\ninformación\\n \\nque\\n \\nguardamos\\nfrom langchain_google_genai  import ChatGoogleGenerativeAI  from dotenv  import load_dotenv  import os   load_dotenv()   api_key  =  os.getenv(\"API_KEY\")   llm  =  ChatGoogleGenerativeAI(      api_key=api_key,      model  =  \"gemini-2.5-flash\",      temperature=0.5 )  7.  Creamos  un  prompt_template  para  nuestro  llm  con  ChatPromptTemplate   from langchain_core.prompts  import ChatPromptTemplate  ai_msg  =  [\"\"]  human_msg  =  []   prompt_template  =  ChatPromptTemplate.from_messages([      (\"system\",\"\"\"      Eres  un  asistente  encargado  de  responder  preguntas  sobre  Langchain.                     Responde  solo  si   {context}  posee  contenido.  Si  el  contexto  esta  vacio,                      responde  \"No  tengo  suficiente  informacion  para  reponder  esa  pregunta\"      \"\"\"),      (\"ai\",\"{ai_msg}\"),      (\"human\",\"{human_msg}\")  ])\\n8.  Casi  terminamos,  ahora  vamos  a  declarar  una  variable  donde  utilizaremos  un  input,  para  \\nprobar\\n \\nel\\n \\nfuncionamiento\\n \\nde\\n \\nnuestro\\n \\nrag.\\n  #  solicitud  del  usuario input_user  =  input(\"Human:  \")  human_msg.append(input_user)  9.  La  solicitud  del  punto  8,  utilizaremos  para  hacer  una  búsqueda  vectorial  con  \\nsimilitary_search.\\n \\nLuego,\\n \\npasaremos\\n \\nun\\n \\ndiccionario\\n \\na\\n \\nnuestro\\n \\nprompt_template\\n \\npara\\n \\nrellenar\\n \\nlas\\n \\nvariables\\n \\nque\\n \\nestá\\n \\nesperando.\\n \\n  docs  =  vector_store.similarity_search(input_user,  k=10)   prompt  =  prompt_template.invoke(  {      \"context\":  docs,      \"ai_msg\":  ai_msg,      \"human_msg\":  human_msg  })  10.  Invocamos  el  modelo  con  el  método  invoke   response  =  llm.invoke(prompt)   print(response.content)    Resultado  obtenido:   ╭─  ~\\\\Desktop\\\\langchain  ╰─   Human:  que  son  los  prompt  templates?    AI:  Los  Prompt  Templates  en  Langchain  son  clases  que  permiten  crear  plantillas  de  mensajes  reutilizables.  Estas  plantillas  pueden  definir  variables  (encerradas  entre  llaves  `{}`)  que  se  llenarán  con  valores  específicos  al  momento  de  invocar  el  modelo.  Existe  también  la  clase  `ChatPromptTemplate`,  que  a  diferencia  de  `PromptTemplate`,  espera  un  diccionario  con  las  variables  declaradas  en  el  template.\\nActividades   Todo  el  contenido  de  la  actividad,  debe  ser  entregado  en  un  júpiter  notebook,  con  sus  \\ncorrespondientes\\n \\nceldas\\n \\nde\\n \\nmarkdown\\n \\npara\\n \\nla\\n \\ndocumentación\\n \\ny\\n \\nceldas\\n \\nde\\n \\ncódigo.\\n \\n1.  Implementación  Práctica  de  RAG:  \\n●  Recrea  el  ejemplo  dado  en  un  Jupyter  Notebook  o  entorno  de  desarrollo  similar,  \\nutilizando\\n \\nlas\\n \\nlibrerías\\n \\nlangchain_community.document_loaders,\\n \\nlangchain_text_splitters,\\n \\nlangchain_ollama,\\n \\nlangchain_chroma,\\n \\ny\\n \\nlangchain_google_genai.\\n ●  Verifica  el  funcionamiento  del  sistema  RAG  respondiendo  a  preguntas  relacionadas  \\ncon\\n \\nel\\n \\ncontenido\\n \\nde\\n \\nun\\n \\ndocumento\\n \\nX\\n \\n(deben\\n \\nproporcionar\\n \\nuno\\n \\nustedes).\\n \\n2.  Exploración  de  Modelos  de  Embedding  y  LLM:  \\n●  Experimenta  con  diferentes  modelos  de  embedding  disponibles  en  langchain_ollama  \\n(además\\n \\nde\\n \\n\"nomic-embed-text\")\\n \\ny\\n \\nanaliza\\n \\ncómo\\n \\nimpactan\\n \\nen\\n \\nla\\n \\ncalidad\\n \\nde\\n \\nla\\n \\nrecuperación\\n \\nde\\n \\ndocumentos.\\n ●  Prueba  con  otros  modelos  de  lenguaje  de  gran  tamaño  (LLM)  compatibles  con  \\nlangchain_google_genai\\n \\no\\n \\nlangchain_ollama\\n \\ny\\n \\ncompara\\n \\nsus\\n \\nrespuestas\\n \\ny\\n \\nrendimiento.\\n \\n3.  Optimización  del  Separador  de  Texto:  \\n●  Modifica  los  parámetros  chunk_size  y  chunk_overlap  del  CharacterTextSplitter  y  \\nobserva\\n \\ncómo\\n \\nafectan\\n \\na\\n \\nla\\n \\ncreación\\n \\nde\\n \\nlos\\n \\ndocumentos\\n \\ny,\\n \\nconsecuentemente,\\n \\na\\n \\nla\\n \\nprecisión\\n \\nde\\n \\nlas\\n \\nrespuestas\\n \\ndel\\n \\nLLM.\\n ●  Investiga  otros  tipos  de  separadores  de  texto  disponibles  en  Langchain  y  evalúa  su  \\nidoneidad\\n \\npara\\n \\ndiferentes\\n \\ntipos\\n \\nde\\n \\ndocumentos.\\n \\n4.  Gestión  de  Bases  de  Datos  Vectoriales:  \\n●  Explora  las  funcionalidades  de  Chroma  para  gestionar  colecciones,  persistencia  de  \\ndatos\\n \\ny\\n \\nrealizar\\n \\nbúsquedas\\n \\nmás\\n \\navanzadas\\n \\n(por\\n \\nejemplo,\\n \\nfiltrado\\n \\nde\\n \\nresultados).\\n ●  Investiga  otras  bases  de  datos  vectoriales  compatibles  con  Langchain  y  considera  sus  \\nventajas\\n \\ny\\n \\ndesventajas\\n \\npara\\n \\ndiferentes\\n \\ncasos\\n \\nde\\n \\nuso.\\n \\n5.  Refinamiento  de  Prompts:  \\n●  Experimenta  con  diferentes  prompt_template  para  el  LLM,  ajustando  las  instrucciones  \\ndel\\n \\nsistema\\n \\ny\\n \\nlos\\n \\nmensajes\\n \\nde\\n \\nIA/humano\\n \\npara\\n \\nmejorar\\n \\nla\\n \\ncalidad\\n \\ny\\n \\nrelevancia\\n \\nde\\n \\nlas\\n \\nrespuestas.\\n ●  Implementa  técnicas  de  ingeniería  de  prompts  para  guiar  al  LLM  a  proporcionar  \\nrespuestas\\n \\nmás\\n \\nprecisas\\n \\ny\\n \\nútiles,\\n \\nincluyendo\\n \\nla\\n \\natribución\\n \\nde\\n \\nfuentes\\n \\ncuando\\n \\nsea\\n \\nposible.')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['371e3635-8615-4121-a86d-59d48e94ddaf',\n",
       " 'a7316757-0881-4681-994e-7053370fcfe4']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = upload_pdf(\"asd.pdf\")\n",
    "texts = text_splitter(loader)\n",
    "vector_store = get_vector_store(\"langchain\")\n",
    "\n",
    "vector_store.add_documents(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df108cc",
   "metadata": {},
   "source": [
    "### Ponemos a prueba nuestro RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "53cecb02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "que es un modelo\n",
      "[Document(id='a7316757-0881-4681-994e-7053370fcfe4', metadata={}, page_content='nuestro\\n \\nllm\\n \\nque\\n \\nresponderá\\n \\npreguntas\\n \\nutilizando\\n \\nla\\n \\ninformación\\n \\nque\\n \\nguardamos\\nfrom langchain_google_genai  import ChatGoogleGenerativeAI  from dotenv  import load_dotenv  import os   load_dotenv()   api_key  =  os.getenv(\"API_KEY\")   llm  =  ChatGoogleGenerativeAI(      api_key=api_key,      model  =  \"gemini-2.5-flash\",      temperature=0.5 )  7.  Creamos  un  prompt_template  para  nuestro  llm  con  ChatPromptTemplate   from langchain_core.prompts  import ChatPromptTemplate  ai_msg  =  [\"\"]  human_msg  =  []   prompt_template  =  ChatPromptTemplate.from_messages([      (\"system\",\"\"\"      Eres  un  asistente  encargado  de  responder  preguntas  sobre  Langchain.                     Responde  solo  si   {context}  posee  contenido.  Si  el  contexto  esta  vacio,                      responde  \"No  tengo  suficiente  informacion  para  reponder  esa  pregunta\"      \"\"\"),      (\"ai\",\"{ai_msg}\"),      (\"human\",\"{human_msg}\")  ])\\n8.  Casi  terminamos,  ahora  vamos  a  declarar  una  variable  donde  utilizaremos  un  input,  para  \\nprobar\\n \\nel\\n \\nfuncionamiento\\n \\nde\\n \\nnuestro\\n \\nrag.\\n  #  solicitud  del  usuario input_user  =  input(\"Human:  \")  human_msg.append(input_user)  9.  La  solicitud  del  punto  8,  utilizaremos  para  hacer  una  búsqueda  vectorial  con  \\nsimilitary_search.\\n \\nLuego,\\n \\npasaremos\\n \\nun\\n \\ndiccionario\\n \\na\\n \\nnuestro\\n \\nprompt_template\\n \\npara\\n \\nrellenar\\n \\nlas\\n \\nvariables\\n \\nque\\n \\nestá\\n \\nesperando.\\n \\n  docs  =  vector_store.similarity_search(input_user,  k=10)   prompt  =  prompt_template.invoke(  {      \"context\":  docs,      \"ai_msg\":  ai_msg,      \"human_msg\":  human_msg  })  10.  Invocamos  el  modelo  con  el  método  invoke   response  =  llm.invoke(prompt)   print(response.content)    Resultado  obtenido:   ╭─  ~\\\\Desktop\\\\langchain  ╰─   Human:  que  son  los  prompt  templates?    AI:  Los  Prompt  Templates  en  Langchain  son  clases  que  permiten  crear  plantillas  de  mensajes  reutilizables.  Estas  plantillas  pueden  definir  variables  (encerradas  entre  llaves  `{}`)  que  se  llenarán  con  valores  específicos  al  momento  de  invocar  el  modelo.  Existe  también  la  clase  `ChatPromptTemplate`,  que  a  diferencia  de  `PromptTemplate`,  espera  un  diccionario  con  las  variables  declaradas  en  el  template.\\nActividades   Todo  el  contenido  de  la  actividad,  debe  ser  entregado  en  un  júpiter  notebook,  con  sus  \\ncorrespondientes\\n \\nceldas\\n \\nde\\n \\nmarkdown\\n \\npara\\n \\nla\\n \\ndocumentación\\n \\ny\\n \\nceldas\\n \\nde\\n \\ncódigo.\\n \\n1.  Implementación  Práctica  de  RAG:  \\n●  Recrea  el  ejemplo  dado  en  un  Jupyter  Notebook  o  entorno  de  desarrollo  similar,  \\nutilizando\\n \\nlas\\n \\nlibrerías\\n \\nlangchain_community.document_loaders,\\n \\nlangchain_text_splitters,\\n \\nlangchain_ollama,\\n \\nlangchain_chroma,\\n \\ny\\n \\nlangchain_google_genai.\\n ●  Verifica  el  funcionamiento  del  sistema  RAG  respondiendo  a  preguntas  relacionadas  \\ncon\\n \\nel\\n \\ncontenido\\n \\nde\\n \\nun\\n \\ndocumento\\n \\nX\\n \\n(deben\\n \\nproporcionar\\n \\nuno\\n \\nustedes).\\n \\n2.  Exploración  de  Modelos  de  Embedding  y  LLM:  \\n●  Experimenta  con  diferentes  modelos  de  embedding  disponibles  en  langchain_ollama  \\n(además\\n \\nde\\n \\n\"nomic-embed-text\")\\n \\ny\\n \\nanaliza\\n \\ncómo\\n \\nimpactan\\n \\nen\\n \\nla\\n \\ncalidad\\n \\nde\\n \\nla\\n \\nrecuperación\\n \\nde\\n \\ndocumentos.\\n ●  Prueba  con  otros  modelos  de  lenguaje  de  gran  tamaño  (LLM)  compatibles  con  \\nlangchain_google_genai\\n \\no\\n \\nlangchain_ollama\\n \\ny\\n \\ncompara\\n \\nsus\\n \\nrespuestas\\n \\ny\\n \\nrendimiento.\\n \\n3.  Optimización  del  Separador  de  Texto:  \\n●  Modifica  los  parámetros  chunk_size  y  chunk_overlap  del  CharacterTextSplitter  y  \\nobserva\\n \\ncómo\\n \\nafectan\\n \\na\\n \\nla\\n \\ncreación\\n \\nde\\n \\nlos\\n \\ndocumentos\\n \\ny,\\n \\nconsecuentemente,\\n \\na\\n \\nla\\n \\nprecisión\\n \\nde\\n \\nlas\\n \\nrespuestas\\n \\ndel\\n \\nLLM.\\n ●  Investiga  otros  tipos  de  separadores  de  texto  disponibles  en  Langchain  y  evalúa  su  \\nidoneidad\\n \\npara\\n \\ndiferentes\\n \\ntipos\\n \\nde\\n \\ndocumentos.\\n \\n4.  Gestión  de  Bases  de  Datos  Vectoriales:  \\n●  Explora  las  funcionalidades  de  Chroma  para  gestionar  colecciones,  persistencia  de  \\ndatos\\n \\ny\\n \\nrealizar\\n \\nbúsquedas\\n \\nmás\\n \\navanzadas\\n \\n(por\\n \\nejemplo,\\n \\nfiltrado\\n \\nde\\n \\nresultados).\\n ●  Investiga  otras  bases  de  datos  vectoriales  compatibles  con  Langchain  y  considera  sus  \\nventajas\\n \\ny\\n \\ndesventajas\\n \\npara\\n \\ndiferentes\\n \\ncasos\\n \\nde\\n \\nuso.\\n \\n5.  Refinamiento  de  Prompts:  \\n●  Experimenta  con  diferentes  prompt_template  para  el  LLM,  ajustando  las  instrucciones  \\ndel\\n \\nsistema\\n \\ny\\n \\nlos\\n \\nmensajes\\n \\nde\\n \\nIA/humano\\n \\npara\\n \\nmejorar\\n \\nla\\n \\ncalidad\\n \\ny\\n \\nrelevancia\\n \\nde\\n \\nlas\\n \\nrespuestas.\\n ●  Implementa  técnicas  de  ingeniería  de  prompts  para  guiar  al  LLM  a  proporcionar  \\nrespuestas\\n \\nmás\\n \\nprecisas\\n \\ny\\n \\nútiles,\\n \\nincluyendo\\n \\nla\\n \\natribución\\n \\nde\\n \\nfuentes\\n \\ncuando\\n \\nsea\\n \\nposible.'), Document(id='371e3635-8615-4121-a86d-59d48e94ddaf', metadata={}, page_content='Introducción  a  RAG   La  generación  aumentada  por  recuperación  (RAG)  es  el  proceso  de  optimización  de  la  salida  \\nde\\n \\nun\\n \\nmodelo\\n \\nde\\n \\nlenguaje\\n \\nde\\n \\ngran\\n \\ntamaño,\\n \\nde\\n \\nmodo\\n \\nque\\n \\nhaga\\n \\nreferencia\\n \\na\\n \\nuna\\n \\nbase\\n \\nde\\n \\nconocimientos\\n \\nautorizada\\n \\nfuera\\n \\nde\\n \\nlos\\n \\norígenes\\n \\nde\\n \\ndatos\\n \\nde\\n \\nentrenamiento\\n \\nantes\\n \\nde\\n \\ngenerar\\n \\nuna\\n \\nrespuesta.\\n \\nLos\\n \\nmodelos\\n \\nde\\n \\nlenguaje\\n \\nde\\n \\ngran\\n \\ntamaño\\n \\n(LLM)\\n \\nse\\n \\nentrenan\\n \\ncon\\n \\nvolúmenes\\n \\nde\\n \\ndatos\\n \\namplios\\n \\ny\\n \\nusan\\n \\nmiles\\n \\nde\\n \\nmillones\\n \\nde\\n \\nparámetros\\n \\npara\\n \\ngenerar\\n \\nresultados\\n \\noriginales\\n \\nen\\n \\ntareas\\n \\ncomo\\n \\nresponder\\n \\npreguntas,\\n \\ntraducir\\n \\nidiomas\\n \\ny\\n \\ncompletar\\n \\nfrases.\\n \\nLa\\n \\nRAG\\n \\nextiende\\n \\nlas\\n \\nya\\n \\npoderosas\\n \\ncapacidades\\n \\nde\\n \\nlos\\n \\nLLM\\n \\na\\n \\ndominios\\n \\nespecíficos\\n \\no\\n \\na\\n \\nla\\n \\nbase\\n \\nde\\n \\nconocimientos\\n \\ninterna\\n \\nde\\n \\nuna\\n \\norganización,\\n \\ntodo\\n \\nello\\n \\nsin\\n \\nla\\n \\nnecesidad\\n \\nde\\n \\nvolver\\n \\na\\n \\nentrenar\\n \\nel\\n \\nmodelo.\\n \\nSe\\n \\ntrata\\n \\nde\\n \\nun\\n \\nmétodo\\n \\nrentable\\n \\npara\\n \\nmejorar\\n \\nlos\\n \\nresultados\\n \\nde\\n \\nlos\\n \\nLLM\\n \\nde\\n \\nmodo\\n \\nque\\n \\nsigan\\n \\nsiendo\\n \\nrelevantes,\\n \\nprecisos\\n \\ny\\n \\nútiles\\n \\nen\\n \\ndiversos\\n \\ncontextos.\\n   ¿Cuáles  son  los  beneficios  de  la  generación  aumentada  por  recuperación?   La  tecnología  RAG  aporta  varios  beneficios  a  los  esfuerzos  de  la  IA  generativa  de  una  \\norganización.\\n  ●  Implementación  rentable:  El  desarrollo  de  chatbots  normalmente  comienza  con  un  \\nmodelo\\n \\nfundacional.\\n \\nLos\\n \\nmodelos\\n \\nfundacionales\\n \\n(FM)\\n \\nson\\n \\nLLM\\n \\naccesibles\\n \\npara\\n \\nAPI\\n \\nentrenados\\n \\nen\\n \\nun\\n \\namplio\\n \\nespectro\\n \\nde\\n \\ndatos\\n \\ngeneralizados\\n \\ny\\n \\nsin\\n \\netiquetar.\\n \\nLos\\n \\ncostos\\n \\ncomputacionales\\n \\ny\\n \\nfinancieros\\n \\nde\\n \\nvolver\\n \\na\\n \\nentrenar\\n \\na\\n \\nlos\\n \\nFM\\n \\npara\\n \\nobtener\\n \\ninformación\\n \\nespecífica\\n \\nde\\n \\nla\\n \\norganización\\n \\no\\n \\ndel\\n \\ndominio\\n \\nson\\n \\naltos.\\n \\nLa\\n \\nRAG\\n \\nes\\n \\nun\\n \\nenfoque\\n \\nmás\\n \\nrentable\\n \\npara\\n \\nintroducir\\n \\nnuevos\\n \\ndatos\\n \\nen\\n \\nel\\n \\nLLM.\\n \\nHace\\n \\nque\\n \\nla\\n \\ntecnología\\n \\nde\\n \\ninteligencia\\n \\nartificial\\n \\ngenerativa\\n \\n(IA\\n \\ngenerativa)\\n \\nsea\\n \\nmás\\n \\naccesible\\n \\ny\\n \\nutilizable.\\n  ●  Información  actual:  Incluso  si  los  orígenes  de  datos  de  entrenamiento  originales  para  \\nun\\n \\nLLM\\n \\nson\\n \\nadecuados\\n \\npara\\n \\nsus\\n \\nnecesidades,\\n \\nes\\n \\ndifícil\\n \\nmantener\\n \\nla\\n \\nrelevancia.\\n \\nLa\\n \\nRAG\\n \\nles\\n \\npermite\\n \\na\\n \\nlos\\n \\ndesarrolladores\\n \\nproporcionar\\n \\nlas\\n \\núltimas\\n \\ninvestigaciones,\\n \\nestadísticas\\n \\no\\n \\nnoticias\\n \\na\\n \\nlos\\n \\nmodelos\\n \\ngenerativos.\\n \\nPueden\\n \\nusar\\n \\nla\\n \\nRAG\\n \\npara\\n \\nconectar\\n \\nel\\n \\nLLM\\n \\nde\\n \\nmanera\\n \\ndirecta\\n \\na\\n \\nredes\\n \\nsociales\\n \\nen\\n \\nvivo,\\n \\nsitios\\n \\nde\\n \\nnoticias\\n \\nu\\n \\notras\\n \\nfuentes\\n \\nde\\n \\ninformación\\n \\nque\\n \\nse\\n \\nactualizan\\n \\ncon\\n \\nfrecuencia.\\n \\nEl\\n \\nLLM\\n \\npuede\\n \\nentonces\\n \\nproporcionar\\n \\nla\\n \\ninformación\\n \\nmás\\n \\nreciente\\n \\na\\n \\nlos\\n \\nusuarios.\\n  ●  Mayor  confianza  de  los  usuarios:  La  RAG  le  permite  al  LLM  presentar  información  \\nprecisa\\n \\ncon\\n \\nla\\n \\natribución\\n \\nde\\n \\nla\\n \\nfuente.\\n \\nLa\\n \\nsalida\\n \\npuede\\n \\nincluir\\n \\ncitas\\n \\no\\n \\nreferencias\\n \\na\\n \\nfuentes.\\n \\nLos\\n \\nusuarios\\n \\ntambién\\n \\npueden\\n \\nbuscar\\n \\nellos\\n \\nmismos\\n \\nlos\\n \\ndocumentos\\n \\nde\\n \\norigen\\n \\nsi\\n \\nnecesitan\\n \\nmás\\n \\naclaraciones\\n \\no\\n \\nmás\\n \\ndetalles.\\n \\nEsto\\n \\npuede\\n \\naumentar\\n \\nla\\n \\nconfianza\\n \\nen\\n \\nsu\\n \\nsolución\\n \\nde\\n \\nIA\\n \\ngenerativa.\\n●  Más  control  para  los  desarrolladores:  Con  la  RAG,  los  desarrolladores  pueden  \\nprobar\\n \\ny\\n \\nmejorar\\n \\nsus\\n \\naplicaciones\\n \\nde\\n \\nchat\\n \\nde\\n \\nmanera\\n \\nmás\\n \\neficiente.\\n \\nPueden\\n \\ncontrolar\\n \\ny\\n \\ncambiar\\n \\nlas\\n \\nfuentes\\n \\nde\\n \\ninformación\\n \\ndel\\n \\nLLM\\n \\npara\\n \\nadaptarse\\n \\na\\n \\nlos\\n \\nrequisitos\\n \\ncambiantes\\n \\no\\n \\nal\\n \\nuso\\n \\nmultifuncional.\\n \\nLos\\n \\ndesarrolladores\\n \\ntambién\\n \\npueden\\n \\nrestringir\\n \\nla\\n \\nrecuperación\\n \\nde\\n \\ninformación\\n \\nconfidencial\\n \\na\\n \\ndiferentes\\n \\nniveles\\n \\nde\\n \\nautorización\\n \\ny\\n \\ngarantizar\\n \\nque\\n \\nel\\n \\nLLM\\n \\ngenere\\n \\nlas\\n \\nrespuestas\\n \\nadecuadas.\\n \\nAdemás,\\n \\ntambién\\n \\npueden\\n \\nsolucionar\\n \\nproblemas\\n \\ny\\n \\nhacer\\n \\ncorrecciones\\n \\nsi\\n \\nel\\n \\nLLM\\n \\nhace\\n \\nreferencia\\n \\na\\n \\nfuentes\\n \\nde\\n \\ninformación\\n \\nincorrectas\\n \\npara\\n \\npreguntas\\n \\nespecíficas.\\n \\nLas\\n \\norganizaciones\\n \\npueden\\n \\nimplementar\\n \\nla\\n \\ntecnología\\n \\nde\\n \\nIA\\n \\ngenerativa\\n \\ncon\\n \\nmayor\\n \\nconfianza\\n \\npara\\n \\nuna\\n \\ngama\\n \\nmás\\n \\namplia\\n \\nde\\n \\naplicaciones.\\n  Implementación  con  Langhain   1.  Cargamos  el  documento  con  PyPDFLoader  y  lo  convertimos  en  un  iterable.  Luego  \\nrecorremos\\n \\nese\\n \\niterable\\n \\ny\\n \\nguardamos\\n \\ncada\\n \\npage\\n \\nen\\n \\nuna\\n \\nvariable.\\n \\n  from langchain_community.document_loaders  import PyPDFLoader  #  cargar  el  documento document  =  PyPDFLoader(\"LANGCHAIN.pdf\")   #  creamos  el  iterador loader  =  document.lazy_load()   text  =  \"\" #  recorremos  el  contenido  del  loader   for page  in loader:       text  +=  page.page_content  2.  Importamos  nuestro  separador  de  texto,  lo  implementamos  y  creamos  los  documentos  que  \\ncargaremos\\n \\nposteriormente\\n \\nen\\n \\nnuestra\\n \\nbase\\n \\nde\\n \\ndatos\\n \\nvectorial:\\n \\n  from langchain_text_splitters  import CharacterTextSplitter  #  creamos  nuestro  separador  de  texto text_splitter  =  CharacterTextSplitter(      chunk_size=2000,      chunk_overlap=100,      separator=\"\\\\n\" )  #  creamos  los  documentos  para  cargar  nuestra  base  de  datos  vectorial texts  =  text_splitter.create_documents([text])\\n3.  Importamos  OllamaEmbedding  desde  langchain_ollama  para  utilizar  el  modelo  de  \\nembedding\\n  from langchain_ollama  import OllamaEmbeddings  #  creamos  la  comunicacion  con  nuestro  modelo  de  embedding embedding  =  OllamaEmbeddings(      model=\"nomic-embed-text\" )     4.  Ahora,  importamos  Chroma  desde  langchain_chroma  y  creamos  nuestra  vector  store   from langchain_chroma  import Chroma   #  declaramos  la  base  de  datos  vectorial vector_store  =  Chroma(      collection_name=\"test\",      embedding_function=embedding,      persist_directory=\"./vectorstore-nomic\" )  5.  Cargamos  los  documentos  que  creamos  en  el  punto  2   #  creamos  los  embeddings  y  lo  guardamos  en  la  base  de  datos  vectorial   vector_store.add_documents(texts)  6.  En  este  punto,  ya  tenemos  documentos  en  nuestra  base  de  datos  vectorial,  por  lo  tanto,  \\nvamos\\n \\na\\n \\ncrear\\n \\nnuestro\\n \\nllm\\n \\nque\\n \\nresponderá\\n \\npreguntas\\n \\nutilizando\\n \\nla\\n \\ninformación\\n \\nque\\n \\nguardamos')]\n",
      "Según el contexto proporcionado:\n",
      "\n",
      "Un modelo puede referirse a un **modelo de lenguaje de gran tamaño (LLM)** o a un **modelo de embedding**.\n",
      "\n",
      "Los **modelos de lenguaje de gran tamaño (LLM)** se entrenan  con volúmenes de datos amplios y usan miles de millones de parámetros para generar resultados originales en tareas como responder preguntas, traducir idiomas y completar frases. Los **modelos fundacionales (FM)** son un tipo de LLM accesibles para API , entrenados en un amplio espectro de datos generalizados y sin etiquetar.\n",
      "\n",
      "Los **modelos de embedding**, como \"nomic-embed-text\" mencionados en el contexto, se utilizan para crear embeddings.  que son los embeddings\n",
      "[Document(id='371e3635-8615-4121-a86d-59d48e94ddaf', metadata={}, page_content='Introducción  a  RAG   La  generación  aumentada  por  recuperación  (RAG)  es  el  proceso  de  optimización  de  la  salida  \\nde\\n \\nun\\n \\nmodelo\\n \\nde\\n \\nlenguaje\\n \\nde\\n \\ngran\\n \\ntamaño,\\n \\nde\\n \\nmodo\\n \\nque\\n \\nhaga\\n \\nreferencia\\n \\na\\n \\nuna\\n \\nbase\\n \\nde\\n \\nconocimientos\\n \\nautorizada\\n \\nfuera\\n \\nde\\n \\nlos\\n \\norígenes\\n \\nde\\n \\ndatos\\n \\nde\\n \\nentrenamiento\\n \\nantes\\n \\nde\\n \\ngenerar\\n \\nuna\\n \\nrespuesta.\\n \\nLos\\n \\nmodelos\\n \\nde\\n \\nlenguaje\\n \\nde\\n \\ngran\\n \\ntamaño\\n \\n(LLM)\\n \\nse\\n \\nentrenan\\n \\ncon\\n \\nvolúmenes\\n \\nde\\n \\ndatos\\n \\namplios\\n \\ny\\n \\nusan\\n \\nmiles\\n \\nde\\n \\nmillones\\n \\nde\\n \\nparámetros\\n \\npara\\n \\ngenerar\\n \\nresultados\\n \\noriginales\\n \\nen\\n \\ntareas\\n \\ncomo\\n \\nresponder\\n \\npreguntas,\\n \\ntraducir\\n \\nidiomas\\n \\ny\\n \\ncompletar\\n \\nfrases.\\n \\nLa\\n \\nRAG\\n \\nextiende\\n \\nlas\\n \\nya\\n \\npoderosas\\n \\ncapacidades\\n \\nde\\n \\nlos\\n \\nLLM\\n \\na\\n \\ndominios\\n \\nespecíficos\\n \\no\\n \\na\\n \\nla\\n \\nbase\\n \\nde\\n \\nconocimientos\\n \\ninterna\\n \\nde\\n \\nuna\\n \\norganización,\\n \\ntodo\\n \\nello\\n \\nsin\\n \\nla\\n \\nnecesidad\\n \\nde\\n \\nvolver\\n \\na\\n \\nentrenar\\n \\nel\\n \\nmodelo.\\n \\nSe\\n \\ntrata\\n \\nde\\n \\nun\\n \\nmétodo\\n \\nrentable\\n \\npara\\n \\nmejorar\\n \\nlos\\n \\nresultados\\n \\nde\\n \\nlos\\n \\nLLM\\n \\nde\\n \\nmodo\\n \\nque\\n \\nsigan\\n \\nsiendo\\n \\nrelevantes,\\n \\nprecisos\\n \\ny\\n \\nútiles\\n \\nen\\n \\ndiversos\\n \\ncontextos.\\n   ¿Cuáles  son  los  beneficios  de  la  generación  aumentada  por  recuperación?   La  tecnología  RAG  aporta  varios  beneficios  a  los  esfuerzos  de  la  IA  generativa  de  una  \\norganización.\\n  ●  Implementación  rentable:  El  desarrollo  de  chatbots  normalmente  comienza  con  un  \\nmodelo\\n \\nfundacional.\\n \\nLos\\n \\nmodelos\\n \\nfundacionales\\n \\n(FM)\\n \\nson\\n \\nLLM\\n \\naccesibles\\n \\npara\\n \\nAPI\\n \\nentrenados\\n \\nen\\n \\nun\\n \\namplio\\n \\nespectro\\n \\nde\\n \\ndatos\\n \\ngeneralizados\\n \\ny\\n \\nsin\\n \\netiquetar.\\n \\nLos\\n \\ncostos\\n \\ncomputacionales\\n \\ny\\n \\nfinancieros\\n \\nde\\n \\nvolver\\n \\na\\n \\nentrenar\\n \\na\\n \\nlos\\n \\nFM\\n \\npara\\n \\nobtener\\n \\ninformación\\n \\nespecífica\\n \\nde\\n \\nla\\n \\norganización\\n \\no\\n \\ndel\\n \\ndominio\\n \\nson\\n \\naltos.\\n \\nLa\\n \\nRAG\\n \\nes\\n \\nun\\n \\nenfoque\\n \\nmás\\n \\nrentable\\n \\npara\\n \\nintroducir\\n \\nnuevos\\n \\ndatos\\n \\nen\\n \\nel\\n \\nLLM.\\n \\nHace\\n \\nque\\n \\nla\\n \\ntecnología\\n \\nde\\n \\ninteligencia\\n \\nartificial\\n \\ngenerativa\\n \\n(IA\\n \\ngenerativa)\\n \\nsea\\n \\nmás\\n \\naccesible\\n \\ny\\n \\nutilizable.\\n  ●  Información  actual:  Incluso  si  los  orígenes  de  datos  de  entrenamiento  originales  para  \\nun\\n \\nLLM\\n \\nson\\n \\nadecuados\\n \\npara\\n \\nsus\\n \\nnecesidades,\\n \\nes\\n \\ndifícil\\n \\nmantener\\n \\nla\\n \\nrelevancia.\\n \\nLa\\n \\nRAG\\n \\nles\\n \\npermite\\n \\na\\n \\nlos\\n \\ndesarrolladores\\n \\nproporcionar\\n \\nlas\\n \\núltimas\\n \\ninvestigaciones,\\n \\nestadísticas\\n \\no\\n \\nnoticias\\n \\na\\n \\nlos\\n \\nmodelos\\n \\ngenerativos.\\n \\nPueden\\n \\nusar\\n \\nla\\n \\nRAG\\n \\npara\\n \\nconectar\\n \\nel\\n \\nLLM\\n \\nde\\n \\nmanera\\n \\ndirecta\\n \\na\\n \\nredes\\n \\nsociales\\n \\nen\\n \\nvivo,\\n \\nsitios\\n \\nde\\n \\nnoticias\\n \\nu\\n \\notras\\n \\nfuentes\\n \\nde\\n \\ninformación\\n \\nque\\n \\nse\\n \\nactualizan\\n \\ncon\\n \\nfrecuencia.\\n \\nEl\\n \\nLLM\\n \\npuede\\n \\nentonces\\n \\nproporcionar\\n \\nla\\n \\ninformación\\n \\nmás\\n \\nreciente\\n \\na\\n \\nlos\\n \\nusuarios.\\n  ●  Mayor  confianza  de  los  usuarios:  La  RAG  le  permite  al  LLM  presentar  información  \\nprecisa\\n \\ncon\\n \\nla\\n \\natribución\\n \\nde\\n \\nla\\n \\nfuente.\\n \\nLa\\n \\nsalida\\n \\npuede\\n \\nincluir\\n \\ncitas\\n \\no\\n \\nreferencias\\n \\na\\n \\nfuentes.\\n \\nLos\\n \\nusuarios\\n \\ntambién\\n \\npueden\\n \\nbuscar\\n \\nellos\\n \\nmismos\\n \\nlos\\n \\ndocumentos\\n \\nde\\n \\norigen\\n \\nsi\\n \\nnecesitan\\n \\nmás\\n \\naclaraciones\\n \\no\\n \\nmás\\n \\ndetalles.\\n \\nEsto\\n \\npuede\\n \\naumentar\\n \\nla\\n \\nconfianza\\n \\nen\\n \\nsu\\n \\nsolución\\n \\nde\\n \\nIA\\n \\ngenerativa.\\n●  Más  control  para  los  desarrolladores:  Con  la  RAG,  los  desarrolladores  pueden  \\nprobar\\n \\ny\\n \\nmejorar\\n \\nsus\\n \\naplicaciones\\n \\nde\\n \\nchat\\n \\nde\\n \\nmanera\\n \\nmás\\n \\neficiente.\\n \\nPueden\\n \\ncontrolar\\n \\ny\\n \\ncambiar\\n \\nlas\\n \\nfuentes\\n \\nde\\n \\ninformación\\n \\ndel\\n \\nLLM\\n \\npara\\n \\nadaptarse\\n \\na\\n \\nlos\\n \\nrequisitos\\n \\ncambiantes\\n \\no\\n \\nal\\n \\nuso\\n \\nmultifuncional.\\n \\nLos\\n \\ndesarrolladores\\n \\ntambién\\n \\npueden\\n \\nrestringir\\n \\nla\\n \\nrecuperación\\n \\nde\\n \\ninformación\\n \\nconfidencial\\n \\na\\n \\ndiferentes\\n \\nniveles\\n \\nde\\n \\nautorización\\n \\ny\\n \\ngarantizar\\n \\nque\\n \\nel\\n \\nLLM\\n \\ngenere\\n \\nlas\\n \\nrespuestas\\n \\nadecuadas.\\n \\nAdemás,\\n \\ntambién\\n \\npueden\\n \\nsolucionar\\n \\nproblemas\\n \\ny\\n \\nhacer\\n \\ncorrecciones\\n \\nsi\\n \\nel\\n \\nLLM\\n \\nhace\\n \\nreferencia\\n \\na\\n \\nfuentes\\n \\nde\\n \\ninformación\\n \\nincorrectas\\n \\npara\\n \\npreguntas\\n \\nespecíficas.\\n \\nLas\\n \\norganizaciones\\n \\npueden\\n \\nimplementar\\n \\nla\\n \\ntecnología\\n \\nde\\n \\nIA\\n \\ngenerativa\\n \\ncon\\n \\nmayor\\n \\nconfianza\\n \\npara\\n \\nuna\\n \\ngama\\n \\nmás\\n \\namplia\\n \\nde\\n \\naplicaciones.\\n  Implementación  con  Langhain   1.  Cargamos  el  documento  con  PyPDFLoader  y  lo  convertimos  en  un  iterable.  Luego  \\nrecorremos\\n \\nese\\n \\niterable\\n \\ny\\n \\nguardamos\\n \\ncada\\n \\npage\\n \\nen\\n \\nuna\\n \\nvariable.\\n \\n  from langchain_community.document_loaders  import PyPDFLoader  #  cargar  el  documento document  =  PyPDFLoader(\"LANGCHAIN.pdf\")   #  creamos  el  iterador loader  =  document.lazy_load()   text  =  \"\" #  recorremos  el  contenido  del  loader   for page  in loader:       text  +=  page.page_content  2.  Importamos  nuestro  separador  de  texto,  lo  implementamos  y  creamos  los  documentos  que  \\ncargaremos\\n \\nposteriormente\\n \\nen\\n \\nnuestra\\n \\nbase\\n \\nde\\n \\ndatos\\n \\nvectorial:\\n \\n  from langchain_text_splitters  import CharacterTextSplitter  #  creamos  nuestro  separador  de  texto text_splitter  =  CharacterTextSplitter(      chunk_size=2000,      chunk_overlap=100,      separator=\"\\\\n\" )  #  creamos  los  documentos  para  cargar  nuestra  base  de  datos  vectorial texts  =  text_splitter.create_documents([text])\\n3.  Importamos  OllamaEmbedding  desde  langchain_ollama  para  utilizar  el  modelo  de  \\nembedding\\n  from langchain_ollama  import OllamaEmbeddings  #  creamos  la  comunicacion  con  nuestro  modelo  de  embedding embedding  =  OllamaEmbeddings(      model=\"nomic-embed-text\" )     4.  Ahora,  importamos  Chroma  desde  langchain_chroma  y  creamos  nuestra  vector  store   from langchain_chroma  import Chroma   #  declaramos  la  base  de  datos  vectorial vector_store  =  Chroma(      collection_name=\"test\",      embedding_function=embedding,      persist_directory=\"./vectorstore-nomic\" )  5.  Cargamos  los  documentos  que  creamos  en  el  punto  2   #  creamos  los  embeddings  y  lo  guardamos  en  la  base  de  datos  vectorial   vector_store.add_documents(texts)  6.  En  este  punto,  ya  tenemos  documentos  en  nuestra  base  de  datos  vectorial,  por  lo  tanto,  \\nvamos\\n \\na\\n \\ncrear\\n \\nnuestro\\n \\nllm\\n \\nque\\n \\nresponderá\\n \\npreguntas\\n \\nutilizando\\n \\nla\\n \\ninformación\\n \\nque\\n \\nguardamos'), Document(id='a7316757-0881-4681-994e-7053370fcfe4', metadata={}, page_content='nuestro\\n \\nllm\\n \\nque\\n \\nresponderá\\n \\npreguntas\\n \\nutilizando\\n \\nla\\n \\ninformación\\n \\nque\\n \\nguardamos\\nfrom langchain_google_genai  import ChatGoogleGenerativeAI  from dotenv  import load_dotenv  import os   load_dotenv()   api_key  =  os.getenv(\"API_KEY\")   llm  =  ChatGoogleGenerativeAI(      api_key=api_key,      model  =  \"gemini-2.5-flash\",      temperature=0.5 )  7.  Creamos  un  prompt_template  para  nuestro  llm  con  ChatPromptTemplate   from langchain_core.prompts  import ChatPromptTemplate  ai_msg  =  [\"\"]  human_msg  =  []   prompt_template  =  ChatPromptTemplate.from_messages([      (\"system\",\"\"\"      Eres  un  asistente  encargado  de  responder  preguntas  sobre  Langchain.                     Responde  solo  si   {context}  posee  contenido.  Si  el  contexto  esta  vacio,                      responde  \"No  tengo  suficiente  informacion  para  reponder  esa  pregunta\"      \"\"\"),      (\"ai\",\"{ai_msg}\"),      (\"human\",\"{human_msg}\")  ])\\n8.  Casi  terminamos,  ahora  vamos  a  declarar  una  variable  donde  utilizaremos  un  input,  para  \\nprobar\\n \\nel\\n \\nfuncionamiento\\n \\nde\\n \\nnuestro\\n \\nrag.\\n  #  solicitud  del  usuario input_user  =  input(\"Human:  \")  human_msg.append(input_user)  9.  La  solicitud  del  punto  8,  utilizaremos  para  hacer  una  búsqueda  vectorial  con  \\nsimilitary_search.\\n \\nLuego,\\n \\npasaremos\\n \\nun\\n \\ndiccionario\\n \\na\\n \\nnuestro\\n \\nprompt_template\\n \\npara\\n \\nrellenar\\n \\nlas\\n \\nvariables\\n \\nque\\n \\nestá\\n \\nesperando.\\n \\n  docs  =  vector_store.similarity_search(input_user,  k=10)   prompt  =  prompt_template.invoke(  {      \"context\":  docs,      \"ai_msg\":  ai_msg,      \"human_msg\":  human_msg  })  10.  Invocamos  el  modelo  con  el  método  invoke   response  =  llm.invoke(prompt)   print(response.content)    Resultado  obtenido:   ╭─  ~\\\\Desktop\\\\langchain  ╰─   Human:  que  son  los  prompt  templates?    AI:  Los  Prompt  Templates  en  Langchain  son  clases  que  permiten  crear  plantillas  de  mensajes  reutilizables.  Estas  plantillas  pueden  definir  variables  (encerradas  entre  llaves  `{}`)  que  se  llenarán  con  valores  específicos  al  momento  de  invocar  el  modelo.  Existe  también  la  clase  `ChatPromptTemplate`,  que  a  diferencia  de  `PromptTemplate`,  espera  un  diccionario  con  las  variables  declaradas  en  el  template.\\nActividades   Todo  el  contenido  de  la  actividad,  debe  ser  entregado  en  un  júpiter  notebook,  con  sus  \\ncorrespondientes\\n \\nceldas\\n \\nde\\n \\nmarkdown\\n \\npara\\n \\nla\\n \\ndocumentación\\n \\ny\\n \\nceldas\\n \\nde\\n \\ncódigo.\\n \\n1.  Implementación  Práctica  de  RAG:  \\n●  Recrea  el  ejemplo  dado  en  un  Jupyter  Notebook  o  entorno  de  desarrollo  similar,  \\nutilizando\\n \\nlas\\n \\nlibrerías\\n \\nlangchain_community.document_loaders,\\n \\nlangchain_text_splitters,\\n \\nlangchain_ollama,\\n \\nlangchain_chroma,\\n \\ny\\n \\nlangchain_google_genai.\\n ●  Verifica  el  funcionamiento  del  sistema  RAG  respondiendo  a  preguntas  relacionadas  \\ncon\\n \\nel\\n \\ncontenido\\n \\nde\\n \\nun\\n \\ndocumento\\n \\nX\\n \\n(deben\\n \\nproporcionar\\n \\nuno\\n \\nustedes).\\n \\n2.  Exploración  de  Modelos  de  Embedding  y  LLM:  \\n●  Experimenta  con  diferentes  modelos  de  embedding  disponibles  en  langchain_ollama  \\n(además\\n \\nde\\n \\n\"nomic-embed-text\")\\n \\ny\\n \\nanaliza\\n \\ncómo\\n \\nimpactan\\n \\nen\\n \\nla\\n \\ncalidad\\n \\nde\\n \\nla\\n \\nrecuperación\\n \\nde\\n \\ndocumentos.\\n ●  Prueba  con  otros  modelos  de  lenguaje  de  gran  tamaño  (LLM)  compatibles  con  \\nlangchain_google_genai\\n \\no\\n \\nlangchain_ollama\\n \\ny\\n \\ncompara\\n \\nsus\\n \\nrespuestas\\n \\ny\\n \\nrendimiento.\\n \\n3.  Optimización  del  Separador  de  Texto:  \\n●  Modifica  los  parámetros  chunk_size  y  chunk_overlap  del  CharacterTextSplitter  y  \\nobserva\\n \\ncómo\\n \\nafectan\\n \\na\\n \\nla\\n \\ncreación\\n \\nde\\n \\nlos\\n \\ndocumentos\\n \\ny,\\n \\nconsecuentemente,\\n \\na\\n \\nla\\n \\nprecisión\\n \\nde\\n \\nlas\\n \\nrespuestas\\n \\ndel\\n \\nLLM.\\n ●  Investiga  otros  tipos  de  separadores  de  texto  disponibles  en  Langchain  y  evalúa  su  \\nidoneidad\\n \\npara\\n \\ndiferentes\\n \\ntipos\\n \\nde\\n \\ndocumentos.\\n \\n4.  Gestión  de  Bases  de  Datos  Vectoriales:  \\n●  Explora  las  funcionalidades  de  Chroma  para  gestionar  colecciones,  persistencia  de  \\ndatos\\n \\ny\\n \\nrealizar\\n \\nbúsquedas\\n \\nmás\\n \\navanzadas\\n \\n(por\\n \\nejemplo,\\n \\nfiltrado\\n \\nde\\n \\nresultados).\\n ●  Investiga  otras  bases  de  datos  vectoriales  compatibles  con  Langchain  y  considera  sus  \\nventajas\\n \\ny\\n \\ndesventajas\\n \\npara\\n \\ndiferentes\\n \\ncasos\\n \\nde\\n \\nuso.\\n \\n5.  Refinamiento  de  Prompts:  \\n●  Experimenta  con  diferentes  prompt_template  para  el  LLM,  ajustando  las  instrucciones  \\ndel\\n \\nsistema\\n \\ny\\n \\nlos\\n \\nmensajes\\n \\nde\\n \\nIA/humano\\n \\npara\\n \\nmejorar\\n \\nla\\n \\ncalidad\\n \\ny\\n \\nrelevancia\\n \\nde\\n \\nlas\\n \\nrespuestas.\\n ●  Implementa  técnicas  de  ingeniería  de  prompts  para  guiar  al  LLM  a  proporcionar  \\nrespuestas\\n \\nmás\\n \\nprecisas\\n \\ny\\n \\nútiles,\\n \\nincluyendo\\n \\nla\\n \\natribución\\n \\nde\\n \\nfuentes\\n \\ncuando\\n \\nsea\\n \\nposible.')]\n",
      "La pregunta excede mi conocimiento  "
     ]
    }
   ],
   "source": [
    "\n",
    "for l in range(2):\n",
    "    input_user = input(\"Human: \")\n",
    "    print(input_user)\n",
    "\n",
    "    docs = retrieval(input_user=input_user)\n",
    "    print(docs)\n",
    "\n",
    "    for chunk in response(input_user=input_user, contexto=docs):\n",
    "        print(chunk, end=\" \", flush=True)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
