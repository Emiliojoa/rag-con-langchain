{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52ca91fe",
   "metadata": {},
   "source": [
    "### Importamos las dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89ae0fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323302d4",
   "metadata": {},
   "source": [
    "### Variables de entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d85b2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"API_KEY\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a057ecce",
   "metadata": {},
   "source": [
    "### Subir un Documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6721387",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def upload_pdf(url: str):        \n",
    "    try:\n",
    "        loader = PyPDFLoader(url)\n",
    "        loader = loader.lazy_load()\n",
    "\n",
    "        text = \"\"\n",
    "\n",
    "        for page in loader: \n",
    "            text += page.page_content + \"\\n\"\n",
    "\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return []\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197416b6",
   "metadata": {},
   "source": [
    "### Text Splitter para separar todo el contenido de mi documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db664495",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def text_splitter(text): \n",
    "\n",
    "    text_splitter = CharacterTextSplitter(\n",
    "    chunk_size = 7000,\n",
    "    chunk_overlap = 200,\n",
    "    separator=\"\\n\"\n",
    ")\n",
    "    texts = text_splitter.create_documents([text])\n",
    "    print(texts)\n",
    "    return texts\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf35666",
   "metadata": {},
   "source": [
    "### Defino el modelo que utilizaré"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90bc142a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = OllamaEmbeddings(\n",
    "    model = \"nomic-embed-text\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193725a0",
   "metadata": {},
   "source": [
    "### Creo mi base de datos vectorial donde se guardará mi embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db426117",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector_store(name_collection: str): \n",
    "    \n",
    "    vector_store = Chroma(\n",
    "    collection_name= name_collection,\n",
    "    embedding_function=embedding,\n",
    "    persist_directory=\"./vectorstore\"\n",
    ")    \n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7da8de",
   "metadata": {},
   "source": [
    "### Creo el retrieval que devolverá la información en una busqueda de similitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f407b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieval(input_user: str): \n",
    "    vector_store = get_vector_store(\"langchain\")\n",
    "    docs = vector_store.similarity_search(input_user)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad17eac4",
   "metadata": {},
   "source": [
    "### Creamos el propt system para el modelo que vamos a utilizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b084557",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "    Eres un asistente encargado de responder preguntas sobre Arquitectura de software y solo debes contestar si el contexto no está vacio.\n",
    "    En caso de que no cuentes con la información solicitada responde \"disculpe, pero la pregunta excede mi conocimiento\" y si te preguntan algo fuera del contexto principal responde \"No estoy programado para eso\".\n",
    "    Utiliza siempre el contexto proporcionado para responder.\n",
    "    contexto = {contexto}\n",
    "    pregunta del usuario: {input_user}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf8720a",
   "metadata": {},
   "source": [
    "### Creamos la función de respuesta que nos comunicará con nuestro agende de IA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d64f263",
   "metadata": {},
   "outputs": [],
   "source": [
    "def response(input_user: str, contexto: str):\n",
    "    llm = ChatGoogleGenerativeAI(\n",
    "    api_key=api_key,\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    temperature= 0.7\n",
    ")\n",
    "\n",
    "    for chunk in llm.stream(prompt.format(contexto=contexto, input_user=input_user)):\n",
    "        yield chunk.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4180cea",
   "metadata": {},
   "source": [
    "### Utilizamos las funciones para cargar el doc, aplicarle el text_splitter y guardar esos datos como embedding en la base de datos vectorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d98d533d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={}, page_content='1. Investiga y explica con tus palabras cada uno de los siguientes conceptos(podés usar libros, videos, artículos, papers o apuntes de clase):○ ¿Qué es una red neuronal artificial?Una red neuronal artificial es una red neuronal que es artificial porque no es real, es creada por nosotros, es decir es creada por el humano pero no de nacimiento sino que fue creado por nosotros (Son nodos que se encargan de realizar una predicción a partir de datos que se le brindan a través de tensores que pueden estar más o menos abstraidos dependiendo de la libreria que se utiliza)○ ¿Qué función cumplen las capas (entrada, ocultas, salida)?Las funciones que cumoplen las capas son:Entrada: Reciben los datos que le mandamos (ej: pixeles de una imagen, números, etc).Oculta: Se encargan de hacer los cálculos y las transformaciones inmediatas.Salida: Devuelve la predicción o un mensaje final de los datos recibidos, como por ej: “gato”, “spam”, “positivo”, etc.○ ¿Qué es un forward pass y por qué es importante?Un \"Forward Pass\" (también conocido como \"Forward Propagation\") es la fase en la que una red neuronal procesa una entrada y genera una salida. Se lo puede entender como el \"camino hacia adelante\" desde la capa de entrada hasta la capa de salida.Es importante porque es el primer paso fundamental en el ciclo de entrenamiento de una red neuronal, después de que la red realiza una predicción mediante el forward pass, esta salida se compara con el valor real para calcular el error o \"loss\"○ ¿Qué es una función de activación? Nombra al menos tres y explicaen qué se diferencian.Una función de activación es una función matemática que va a decidir si una neurona está activa o no, es decir, cuanta cantidad de la información recibida debe pasar a la siguiente capa. Sin una función de activación, la red neuronal no podría aprender relaciones complejas ni resolver problemas como clasificación lineal.Relu: Trabaja con datos de entrenamiento más complejo donde hay mucha informaciónSigmoid: Trabaja con numeros binarios dando una respuesta como 1 o 0Softmax: Se utiliza en la salida de una red neuronal y es multiclase haciendo que todos los resultados de analizar la información den como resultado 1.○ ¿Qué es una función de pérdida (loss function) y para qué seutiliza? El loss o función de pérdida mide que tan mal está prediciendo una red neuronal. Se encarga de decir a la red en cuanto se está equivocando.compara la salida predicha por la red (ŷ) con el valor real (y) y devuelve un numero:\\n. Si el número es grande, la predicción es mala.. Si el número es cero o pequeño, la predicción es buena.○ ¿Qué es la backpropagation? ¿Cómo se relaciona con el cálculo dederivadas?La Backpropagation es un algoritmo clave que le permite a una red neuronal aprender, funcionando como su sistema de corrección de errores, después de que la red hace una predicción y se calcula el error (o \"loss\"), la Backpropagation calcula cuánto contribuyó cada \"peso\" al error. Esto se relaciona con el cálculo de derivadas porque utiliza \"gradientes\" para saber la dirección y la intensidad en que deben ajustarse los pesos y sesgos.2. Elegí un optimizador (por ejemplo: Adam o SGD) e investiga:Elegimos el optimizador Adagrad ○ ¿Quién lo propuso? (Nombre y año si está disponible)Lo propusieron John Duchi, Elad Hazan y Yoram Singer en el año 2011\"Adaptive Subgradient Methods for Online Learning and Stochastic Optimization\"○ ¿En qué tipo de problemas suele usarse?Se utiliza principalmente en problemas de aprendizaje automático donde los datos son dispersos, como en el procesamiento de lenguaje natural (NLP), la clasificación de texto, la recomendación de sistemas y el aprendizaje en redes neuronales con características de alta dimensionalidad. Su principal ventaja es que adapta automáticamente la tasa de aprendizaje para cada parámetro, asignando tasas más pequeñas a parámetros con gradientes frecuentes y mayores a aquellos con gradientes menos frecuentes, lo que lo hace especialmente efectivo en conjuntos de datos con características esparsas o cuando los gradientes varían significativamente en magnitud.○ ¿Cuáles son sus ventajas y desventajas?Como ventajas:● tiene una tasa de aprendizaje adaptativa que significa que los parametros que cambian mucho se actualizan menos y los que cambian poco se actualizan más●Buenardo para datos dispersos como por ejemplo para modelos de lenguaje naturalComo desventajas: ●No es recomendable usarlo para modelos grandes o para entrenamientos largos●Una vez que deja de aprender se suele quedar atascado3. Reflexión crítica:¿Por qué pensás que el proceso de entrenamiento en redes neuronalesnecesita tantas etapas?Creo que necesita varias etapas porque hay que asegurarse de que los datos de entrenamiento pasen por un camino más limpio para obtener una salida esperada, ya sea limpiando los set de datos, ajustando tamaños de datos de entrenamiento como los de testing, también verificar que los datos de perdida sean los adecuados y evitar anomalias en el entrenamiento de la red neuronal.\\n¿Qué pasaría si no existiera el paso deretropropagación?Quizás cuando el entrenamiento no salga como esperamos los datos de salida del modelo estén más sesgados por datos que no son beneficiosos, por lo tanto el modelo respondería pero de forma erronea.También el aprendizaje sería muy lento o quizás inutil porque se tendria que usar métodos básicos como prueba y error, lo que haría que el entrenamiento sea muy impreciso o directamente inviable en redes grandes.La razón por la que debe de necesitar tantas etapas es porque aprende de apoco y en cada etapa la red ajusta los pesos y sesgos para acercarse a la solución correcta. Esto se repite muchas veces (épocas) porque los datos al ser muchos o complejos se necesitan verlos varias veces para reconocer patrones, minimizar errores , entre otros.')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['33f8106e-dbf7-4d19-abda-0fd91dfb6e3c']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = upload_pdf(\"mi_tp.pdf\")\n",
    "texts = text_splitter(loader)\n",
    "vector_store = get_vector_store(\"langchain\")\n",
    "\n",
    "vector_store.add_documents(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df108cc",
   "metadata": {},
   "source": [
    "### Ponemos a prueba nuestro RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "53cecb02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "que es un neurona\n",
      "[Document(id='33f8106e-dbf7-4d19-abda-0fd91dfb6e3c', metadata={}, page_content='1. Investiga y explica con tus palabras cada uno de los siguientes conceptos(podés usar libros, videos, artículos, papers o apuntes de clase):○ ¿Qué es una red neuronal artificial?Una red neuronal artificial es una red neuronal que es artificial porque no es real, es creada por nosotros, es decir es creada por el humano pero no de nacimiento sino que fue creado por nosotros (Son nodos que se encargan de realizar una predicción a partir de datos que se le brindan a través de tensores que pueden estar más o menos abstraidos dependiendo de la libreria que se utiliza)○ ¿Qué función cumplen las capas (entrada, ocultas, salida)?Las funciones que cumoplen las capas son:Entrada: Reciben los datos que le mandamos (ej: pixeles de una imagen, números, etc).Oculta: Se encargan de hacer los cálculos y las transformaciones inmediatas.Salida: Devuelve la predicción o un mensaje final de los datos recibidos, como por ej: “gato”, “spam”, “positivo”, etc.○ ¿Qué es un forward pass y por qué es importante?Un \"Forward Pass\" (también conocido como \"Forward Propagation\") es la fase en la que una red neuronal procesa una entrada y genera una salida. Se lo puede entender como el \"camino hacia adelante\" desde la capa de entrada hasta la capa de salida.Es importante porque es el primer paso fundamental en el ciclo de entrenamiento de una red neuronal, después de que la red realiza una predicción mediante el forward pass, esta salida se compara con el valor real para calcular el error o \"loss\"○ ¿Qué es una función de activación? Nombra al menos tres y explicaen qué se diferencian.Una función de activación es una función matemática que va a decidir si una neurona está activa o no, es decir, cuanta cantidad de la información recibida debe pasar a la siguiente capa. Sin una función de activación, la red neuronal no podría aprender relaciones complejas ni resolver problemas como clasificación lineal.Relu: Trabaja con datos de entrenamiento más complejo donde hay mucha informaciónSigmoid: Trabaja con numeros binarios dando una respuesta como 1 o 0Softmax: Se utiliza en la salida de una red neuronal y es multiclase haciendo que todos los resultados de analizar la información den como resultado 1.○ ¿Qué es una función de pérdida (loss function) y para qué seutiliza? El loss o función de pérdida mide que tan mal está prediciendo una red neuronal. Se encarga de decir a la red en cuanto se está equivocando.compara la salida predicha por la red (ŷ) con el valor real (y) y devuelve un numero:\\n. Si el número es grande, la predicción es mala.. Si el número es cero o pequeño, la predicción es buena.○ ¿Qué es la backpropagation? ¿Cómo se relaciona con el cálculo dederivadas?La Backpropagation es un algoritmo clave que le permite a una red neuronal aprender, funcionando como su sistema de corrección de errores, después de que la red hace una predicción y se calcula el error (o \"loss\"), la Backpropagation calcula cuánto contribuyó cada \"peso\" al error. Esto se relaciona con el cálculo de derivadas porque utiliza \"gradientes\" para saber la dirección y la intensidad en que deben ajustarse los pesos y sesgos.2. Elegí un optimizador (por ejemplo: Adam o SGD) e investiga:Elegimos el optimizador Adagrad ○ ¿Quién lo propuso? (Nombre y año si está disponible)Lo propusieron John Duchi, Elad Hazan y Yoram Singer en el año 2011\"Adaptive Subgradient Methods for Online Learning and Stochastic Optimization\"○ ¿En qué tipo de problemas suele usarse?Se utiliza principalmente en problemas de aprendizaje automático donde los datos son dispersos, como en el procesamiento de lenguaje natural (NLP), la clasificación de texto, la recomendación de sistemas y el aprendizaje en redes neuronales con características de alta dimensionalidad. Su principal ventaja es que adapta automáticamente la tasa de aprendizaje para cada parámetro, asignando tasas más pequeñas a parámetros con gradientes frecuentes y mayores a aquellos con gradientes menos frecuentes, lo que lo hace especialmente efectivo en conjuntos de datos con características esparsas o cuando los gradientes varían significativamente en magnitud.○ ¿Cuáles son sus ventajas y desventajas?Como ventajas:● tiene una tasa de aprendizaje adaptativa que significa que los parametros que cambian mucho se actualizan menos y los que cambian poco se actualizan más●Buenardo para datos dispersos como por ejemplo para modelos de lenguaje naturalComo desventajas: ●No es recomendable usarlo para modelos grandes o para entrenamientos largos●Una vez que deja de aprender se suele quedar atascado3. Reflexión crítica:¿Por qué pensás que el proceso de entrenamiento en redes neuronalesnecesita tantas etapas?Creo que necesita varias etapas porque hay que asegurarse de que los datos de entrenamiento pasen por un camino más limpio para obtener una salida esperada, ya sea limpiando los set de datos, ajustando tamaños de datos de entrenamiento como los de testing, también verificar que los datos de perdida sean los adecuados y evitar anomalias en el entrenamiento de la red neuronal.\\n¿Qué pasaría si no existiera el paso deretropropagación?Quizás cuando el entrenamiento no salga como esperamos los datos de salida del modelo estén más sesgados por datos que no son beneficiosos, por lo tanto el modelo respondería pero de forma erronea.También el aprendizaje sería muy lento o quizás inutil porque se tendria que usar métodos básicos como prueba y error, lo que haría que el entrenamiento sea muy impreciso o directamente inviable en redes grandes.La razón por la que debe de necesitar tantas etapas es porque aprende de apoco y en cada etapa la red ajusta los pesos y sesgos para acercarse a la solución correcta. Esto se repite muchas veces (épocas) porque los datos al ser muchos o complejos se necesitan verlos varias veces para reconocer patrones, minimizar errores , entre otros.'), Document(id='371e3635-8615-4121-a86d-59d48e94ddaf', metadata={}, page_content='Introducción  a  RAG   La  generación  aumentada  por  recuperación  (RAG)  es  el  proceso  de  optimización  de  la  salida  \\nde\\n \\nun\\n \\nmodelo\\n \\nde\\n \\nlenguaje\\n \\nde\\n \\ngran\\n \\ntamaño,\\n \\nde\\n \\nmodo\\n \\nque\\n \\nhaga\\n \\nreferencia\\n \\na\\n \\nuna\\n \\nbase\\n \\nde\\n \\nconocimientos\\n \\nautorizada\\n \\nfuera\\n \\nde\\n \\nlos\\n \\norígenes\\n \\nde\\n \\ndatos\\n \\nde\\n \\nentrenamiento\\n \\nantes\\n \\nde\\n \\ngenerar\\n \\nuna\\n \\nrespuesta.\\n \\nLos\\n \\nmodelos\\n \\nde\\n \\nlenguaje\\n \\nde\\n \\ngran\\n \\ntamaño\\n \\n(LLM)\\n \\nse\\n \\nentrenan\\n \\ncon\\n \\nvolúmenes\\n \\nde\\n \\ndatos\\n \\namplios\\n \\ny\\n \\nusan\\n \\nmiles\\n \\nde\\n \\nmillones\\n \\nde\\n \\nparámetros\\n \\npara\\n \\ngenerar\\n \\nresultados\\n \\noriginales\\n \\nen\\n \\ntareas\\n \\ncomo\\n \\nresponder\\n \\npreguntas,\\n \\ntraducir\\n \\nidiomas\\n \\ny\\n \\ncompletar\\n \\nfrases.\\n \\nLa\\n \\nRAG\\n \\nextiende\\n \\nlas\\n \\nya\\n \\npoderosas\\n \\ncapacidades\\n \\nde\\n \\nlos\\n \\nLLM\\n \\na\\n \\ndominios\\n \\nespecíficos\\n \\no\\n \\na\\n \\nla\\n \\nbase\\n \\nde\\n \\nconocimientos\\n \\ninterna\\n \\nde\\n \\nuna\\n \\norganización,\\n \\ntodo\\n \\nello\\n \\nsin\\n \\nla\\n \\nnecesidad\\n \\nde\\n \\nvolver\\n \\na\\n \\nentrenar\\n \\nel\\n \\nmodelo.\\n \\nSe\\n \\ntrata\\n \\nde\\n \\nun\\n \\nmétodo\\n \\nrentable\\n \\npara\\n \\nmejorar\\n \\nlos\\n \\nresultados\\n \\nde\\n \\nlos\\n \\nLLM\\n \\nde\\n \\nmodo\\n \\nque\\n \\nsigan\\n \\nsiendo\\n \\nrelevantes,\\n \\nprecisos\\n \\ny\\n \\nútiles\\n \\nen\\n \\ndiversos\\n \\ncontextos.\\n   ¿Cuáles  son  los  beneficios  de  la  generación  aumentada  por  recuperación?   La  tecnología  RAG  aporta  varios  beneficios  a  los  esfuerzos  de  la  IA  generativa  de  una  \\norganización.\\n  ●  Implementación  rentable:  El  desarrollo  de  chatbots  normalmente  comienza  con  un  \\nmodelo\\n \\nfundacional.\\n \\nLos\\n \\nmodelos\\n \\nfundacionales\\n \\n(FM)\\n \\nson\\n \\nLLM\\n \\naccesibles\\n \\npara\\n \\nAPI\\n \\nentrenados\\n \\nen\\n \\nun\\n \\namplio\\n \\nespectro\\n \\nde\\n \\ndatos\\n \\ngeneralizados\\n \\ny\\n \\nsin\\n \\netiquetar.\\n \\nLos\\n \\ncostos\\n \\ncomputacionales\\n \\ny\\n \\nfinancieros\\n \\nde\\n \\nvolver\\n \\na\\n \\nentrenar\\n \\na\\n \\nlos\\n \\nFM\\n \\npara\\n \\nobtener\\n \\ninformación\\n \\nespecífica\\n \\nde\\n \\nla\\n \\norganización\\n \\no\\n \\ndel\\n \\ndominio\\n \\nson\\n \\naltos.\\n \\nLa\\n \\nRAG\\n \\nes\\n \\nun\\n \\nenfoque\\n \\nmás\\n \\nrentable\\n \\npara\\n \\nintroducir\\n \\nnuevos\\n \\ndatos\\n \\nen\\n \\nel\\n \\nLLM.\\n \\nHace\\n \\nque\\n \\nla\\n \\ntecnología\\n \\nde\\n \\ninteligencia\\n \\nartificial\\n \\ngenerativa\\n \\n(IA\\n \\ngenerativa)\\n \\nsea\\n \\nmás\\n \\naccesible\\n \\ny\\n \\nutilizable.\\n  ●  Información  actual:  Incluso  si  los  orígenes  de  datos  de  entrenamiento  originales  para  \\nun\\n \\nLLM\\n \\nson\\n \\nadecuados\\n \\npara\\n \\nsus\\n \\nnecesidades,\\n \\nes\\n \\ndifícil\\n \\nmantener\\n \\nla\\n \\nrelevancia.\\n \\nLa\\n \\nRAG\\n \\nles\\n \\npermite\\n \\na\\n \\nlos\\n \\ndesarrolladores\\n \\nproporcionar\\n \\nlas\\n \\núltimas\\n \\ninvestigaciones,\\n \\nestadísticas\\n \\no\\n \\nnoticias\\n \\na\\n \\nlos\\n \\nmodelos\\n \\ngenerativos.\\n \\nPueden\\n \\nusar\\n \\nla\\n \\nRAG\\n \\npara\\n \\nconectar\\n \\nel\\n \\nLLM\\n \\nde\\n \\nmanera\\n \\ndirecta\\n \\na\\n \\nredes\\n \\nsociales\\n \\nen\\n \\nvivo,\\n \\nsitios\\n \\nde\\n \\nnoticias\\n \\nu\\n \\notras\\n \\nfuentes\\n \\nde\\n \\ninformación\\n \\nque\\n \\nse\\n \\nactualizan\\n \\ncon\\n \\nfrecuencia.\\n \\nEl\\n \\nLLM\\n \\npuede\\n \\nentonces\\n \\nproporcionar\\n \\nla\\n \\ninformación\\n \\nmás\\n \\nreciente\\n \\na\\n \\nlos\\n \\nusuarios.\\n  ●  Mayor  confianza  de  los  usuarios:  La  RAG  le  permite  al  LLM  presentar  información  \\nprecisa\\n \\ncon\\n \\nla\\n \\natribución\\n \\nde\\n \\nla\\n \\nfuente.\\n \\nLa\\n \\nsalida\\n \\npuede\\n \\nincluir\\n \\ncitas\\n \\no\\n \\nreferencias\\n \\na\\n \\nfuentes.\\n \\nLos\\n \\nusuarios\\n \\ntambién\\n \\npueden\\n \\nbuscar\\n \\nellos\\n \\nmismos\\n \\nlos\\n \\ndocumentos\\n \\nde\\n \\norigen\\n \\nsi\\n \\nnecesitan\\n \\nmás\\n \\naclaraciones\\n \\no\\n \\nmás\\n \\ndetalles.\\n \\nEsto\\n \\npuede\\n \\naumentar\\n \\nla\\n \\nconfianza\\n \\nen\\n \\nsu\\n \\nsolución\\n \\nde\\n \\nIA\\n \\ngenerativa.\\n●  Más  control  para  los  desarrolladores:  Con  la  RAG,  los  desarrolladores  pueden  \\nprobar\\n \\ny\\n \\nmejorar\\n \\nsus\\n \\naplicaciones\\n \\nde\\n \\nchat\\n \\nde\\n \\nmanera\\n \\nmás\\n \\neficiente.\\n \\nPueden\\n \\ncontrolar\\n \\ny\\n \\ncambiar\\n \\nlas\\n \\nfuentes\\n \\nde\\n \\ninformación\\n \\ndel\\n \\nLLM\\n \\npara\\n \\nadaptarse\\n \\na\\n \\nlos\\n \\nrequisitos\\n \\ncambiantes\\n \\no\\n \\nal\\n \\nuso\\n \\nmultifuncional.\\n \\nLos\\n \\ndesarrolladores\\n \\ntambién\\n \\npueden\\n \\nrestringir\\n \\nla\\n \\nrecuperación\\n \\nde\\n \\ninformación\\n \\nconfidencial\\n \\na\\n \\ndiferentes\\n \\nniveles\\n \\nde\\n \\nautorización\\n \\ny\\n \\ngarantizar\\n \\nque\\n \\nel\\n \\nLLM\\n \\ngenere\\n \\nlas\\n \\nrespuestas\\n \\nadecuadas.\\n \\nAdemás,\\n \\ntambién\\n \\npueden\\n \\nsolucionar\\n \\nproblemas\\n \\ny\\n \\nhacer\\n \\ncorrecciones\\n \\nsi\\n \\nel\\n \\nLLM\\n \\nhace\\n \\nreferencia\\n \\na\\n \\nfuentes\\n \\nde\\n \\ninformación\\n \\nincorrectas\\n \\npara\\n \\npreguntas\\n \\nespecíficas.\\n \\nLas\\n \\norganizaciones\\n \\npueden\\n \\nimplementar\\n \\nla\\n \\ntecnología\\n \\nde\\n \\nIA\\n \\ngenerativa\\n \\ncon\\n \\nmayor\\n \\nconfianza\\n \\npara\\n \\nuna\\n \\ngama\\n \\nmás\\n \\namplia\\n \\nde\\n \\naplicaciones.\\n  Implementación  con  Langhain   1.  Cargamos  el  documento  con  PyPDFLoader  y  lo  convertimos  en  un  iterable.  Luego  \\nrecorremos\\n \\nese\\n \\niterable\\n \\ny\\n \\nguardamos\\n \\ncada\\n \\npage\\n \\nen\\n \\nuna\\n \\nvariable.\\n \\n  from langchain_community.document_loaders  import PyPDFLoader  #  cargar  el  documento document  =  PyPDFLoader(\"LANGCHAIN.pdf\")   #  creamos  el  iterador loader  =  document.lazy_load()   text  =  \"\" #  recorremos  el  contenido  del  loader   for page  in loader:       text  +=  page.page_content  2.  Importamos  nuestro  separador  de  texto,  lo  implementamos  y  creamos  los  documentos  que  \\ncargaremos\\n \\nposteriormente\\n \\nen\\n \\nnuestra\\n \\nbase\\n \\nde\\n \\ndatos\\n \\nvectorial:\\n \\n  from langchain_text_splitters  import CharacterTextSplitter  #  creamos  nuestro  separador  de  texto text_splitter  =  CharacterTextSplitter(      chunk_size=2000,      chunk_overlap=100,      separator=\"\\\\n\" )  #  creamos  los  documentos  para  cargar  nuestra  base  de  datos  vectorial texts  =  text_splitter.create_documents([text])\\n3.  Importamos  OllamaEmbedding  desde  langchain_ollama  para  utilizar  el  modelo  de  \\nembedding\\n  from langchain_ollama  import OllamaEmbeddings  #  creamos  la  comunicacion  con  nuestro  modelo  de  embedding embedding  =  OllamaEmbeddings(      model=\"nomic-embed-text\" )     4.  Ahora,  importamos  Chroma  desde  langchain_chroma  y  creamos  nuestra  vector  store   from langchain_chroma  import Chroma   #  declaramos  la  base  de  datos  vectorial vector_store  =  Chroma(      collection_name=\"test\",      embedding_function=embedding,      persist_directory=\"./vectorstore-nomic\" )  5.  Cargamos  los  documentos  que  creamos  en  el  punto  2   #  creamos  los  embeddings  y  lo  guardamos  en  la  base  de  datos  vectorial   vector_store.add_documents(texts)  6.  En  este  punto,  ya  tenemos  documentos  en  nuestra  base  de  datos  vectorial,  por  lo  tanto,  \\nvamos\\n \\na\\n \\ncrear\\n \\nnuestro\\n \\nllm\\n \\nque\\n \\nresponderá\\n \\npreguntas\\n \\nutilizando\\n \\nla\\n \\ninformación\\n \\nque\\n \\nguardamos'), Document(id='a7316757-0881-4681-994e-7053370fcfe4', metadata={}, page_content='nuestro\\n \\nllm\\n \\nque\\n \\nresponderá\\n \\npreguntas\\n \\nutilizando\\n \\nla\\n \\ninformación\\n \\nque\\n \\nguardamos\\nfrom langchain_google_genai  import ChatGoogleGenerativeAI  from dotenv  import load_dotenv  import os   load_dotenv()   api_key  =  os.getenv(\"API_KEY\")   llm  =  ChatGoogleGenerativeAI(      api_key=api_key,      model  =  \"gemini-2.5-flash\",      temperature=0.5 )  7.  Creamos  un  prompt_template  para  nuestro  llm  con  ChatPromptTemplate   from langchain_core.prompts  import ChatPromptTemplate  ai_msg  =  [\"\"]  human_msg  =  []   prompt_template  =  ChatPromptTemplate.from_messages([      (\"system\",\"\"\"      Eres  un  asistente  encargado  de  responder  preguntas  sobre  Langchain.                     Responde  solo  si   {context}  posee  contenido.  Si  el  contexto  esta  vacio,                      responde  \"No  tengo  suficiente  informacion  para  reponder  esa  pregunta\"      \"\"\"),      (\"ai\",\"{ai_msg}\"),      (\"human\",\"{human_msg}\")  ])\\n8.  Casi  terminamos,  ahora  vamos  a  declarar  una  variable  donde  utilizaremos  un  input,  para  \\nprobar\\n \\nel\\n \\nfuncionamiento\\n \\nde\\n \\nnuestro\\n \\nrag.\\n  #  solicitud  del  usuario input_user  =  input(\"Human:  \")  human_msg.append(input_user)  9.  La  solicitud  del  punto  8,  utilizaremos  para  hacer  una  búsqueda  vectorial  con  \\nsimilitary_search.\\n \\nLuego,\\n \\npasaremos\\n \\nun\\n \\ndiccionario\\n \\na\\n \\nnuestro\\n \\nprompt_template\\n \\npara\\n \\nrellenar\\n \\nlas\\n \\nvariables\\n \\nque\\n \\nestá\\n \\nesperando.\\n \\n  docs  =  vector_store.similarity_search(input_user,  k=10)   prompt  =  prompt_template.invoke(  {      \"context\":  docs,      \"ai_msg\":  ai_msg,      \"human_msg\":  human_msg  })  10.  Invocamos  el  modelo  con  el  método  invoke   response  =  llm.invoke(prompt)   print(response.content)    Resultado  obtenido:   ╭─  ~\\\\Desktop\\\\langchain  ╰─   Human:  que  son  los  prompt  templates?    AI:  Los  Prompt  Templates  en  Langchain  son  clases  que  permiten  crear  plantillas  de  mensajes  reutilizables.  Estas  plantillas  pueden  definir  variables  (encerradas  entre  llaves  `{}`)  que  se  llenarán  con  valores  específicos  al  momento  de  invocar  el  modelo.  Existe  también  la  clase  `ChatPromptTemplate`,  que  a  diferencia  de  `PromptTemplate`,  espera  un  diccionario  con  las  variables  declaradas  en  el  template.\\nActividades   Todo  el  contenido  de  la  actividad,  debe  ser  entregado  en  un  júpiter  notebook,  con  sus  \\ncorrespondientes\\n \\nceldas\\n \\nde\\n \\nmarkdown\\n \\npara\\n \\nla\\n \\ndocumentación\\n \\ny\\n \\nceldas\\n \\nde\\n \\ncódigo.\\n \\n1.  Implementación  Práctica  de  RAG:  \\n●  Recrea  el  ejemplo  dado  en  un  Jupyter  Notebook  o  entorno  de  desarrollo  similar,  \\nutilizando\\n \\nlas\\n \\nlibrerías\\n \\nlangchain_community.document_loaders,\\n \\nlangchain_text_splitters,\\n \\nlangchain_ollama,\\n \\nlangchain_chroma,\\n \\ny\\n \\nlangchain_google_genai.\\n ●  Verifica  el  funcionamiento  del  sistema  RAG  respondiendo  a  preguntas  relacionadas  \\ncon\\n \\nel\\n \\ncontenido\\n \\nde\\n \\nun\\n \\ndocumento\\n \\nX\\n \\n(deben\\n \\nproporcionar\\n \\nuno\\n \\nustedes).\\n \\n2.  Exploración  de  Modelos  de  Embedding  y  LLM:  \\n●  Experimenta  con  diferentes  modelos  de  embedding  disponibles  en  langchain_ollama  \\n(además\\n \\nde\\n \\n\"nomic-embed-text\")\\n \\ny\\n \\nanaliza\\n \\ncómo\\n \\nimpactan\\n \\nen\\n \\nla\\n \\ncalidad\\n \\nde\\n \\nla\\n \\nrecuperación\\n \\nde\\n \\ndocumentos.\\n ●  Prueba  con  otros  modelos  de  lenguaje  de  gran  tamaño  (LLM)  compatibles  con  \\nlangchain_google_genai\\n \\no\\n \\nlangchain_ollama\\n \\ny\\n \\ncompara\\n \\nsus\\n \\nrespuestas\\n \\ny\\n \\nrendimiento.\\n \\n3.  Optimización  del  Separador  de  Texto:  \\n●  Modifica  los  parámetros  chunk_size  y  chunk_overlap  del  CharacterTextSplitter  y  \\nobserva\\n \\ncómo\\n \\nafectan\\n \\na\\n \\nla\\n \\ncreación\\n \\nde\\n \\nlos\\n \\ndocumentos\\n \\ny,\\n \\nconsecuentemente,\\n \\na\\n \\nla\\n \\nprecisión\\n \\nde\\n \\nlas\\n \\nrespuestas\\n \\ndel\\n \\nLLM.\\n ●  Investiga  otros  tipos  de  separadores  de  texto  disponibles  en  Langchain  y  evalúa  su  \\nidoneidad\\n \\npara\\n \\ndiferentes\\n \\ntipos\\n \\nde\\n \\ndocumentos.\\n \\n4.  Gestión  de  Bases  de  Datos  Vectoriales:  \\n●  Explora  las  funcionalidades  de  Chroma  para  gestionar  colecciones,  persistencia  de  \\ndatos\\n \\ny\\n \\nrealizar\\n \\nbúsquedas\\n \\nmás\\n \\navanzadas\\n \\n(por\\n \\nejemplo,\\n \\nfiltrado\\n \\nde\\n \\nresultados).\\n ●  Investiga  otras  bases  de  datos  vectoriales  compatibles  con  Langchain  y  considera  sus  \\nventajas\\n \\ny\\n \\ndesventajas\\n \\npara\\n \\ndiferentes\\n \\ncasos\\n \\nde\\n \\nuso.\\n \\n5.  Refinamiento  de  Prompts:  \\n●  Experimenta  con  diferentes  prompt_template  para  el  LLM,  ajustando  las  instrucciones  \\ndel\\n \\nsistema\\n \\ny\\n \\nlos\\n \\nmensajes\\n \\nde\\n \\nIA/humano\\n \\npara\\n \\nmejorar\\n \\nla\\n \\ncalidad\\n \\ny\\n \\nrelevancia\\n \\nde\\n \\nlas\\n \\nrespuestas.\\n ●  Implementa  técnicas  de  ingeniería  de  prompts  para  guiar  al  LLM  a  proporcionar  \\nrespuestas\\n \\nmás\\n \\nprecisas\\n \\ny\\n \\nútiles,\\n \\nincluyendo\\n \\nla\\n \\natribución\\n \\nde\\n \\nfuentes\\n \\ncuando\\n \\nsea\\n \\nposible.')]\n",
      "Una neurona, en el contexto de una red neuronal artificial, se puede entender como uno de los \"nodos\" que componen dicha red. Estos nodos se encargan de realizar una predicción a partir de los datos que se le  brindan a través de tensores.  que son los nodos\n",
      "[Document(id='371e3635-8615-4121-a86d-59d48e94ddaf', metadata={}, page_content='Introducción  a  RAG   La  generación  aumentada  por  recuperación  (RAG)  es  el  proceso  de  optimización  de  la  salida  \\nde\\n \\nun\\n \\nmodelo\\n \\nde\\n \\nlenguaje\\n \\nde\\n \\ngran\\n \\ntamaño,\\n \\nde\\n \\nmodo\\n \\nque\\n \\nhaga\\n \\nreferencia\\n \\na\\n \\nuna\\n \\nbase\\n \\nde\\n \\nconocimientos\\n \\nautorizada\\n \\nfuera\\n \\nde\\n \\nlos\\n \\norígenes\\n \\nde\\n \\ndatos\\n \\nde\\n \\nentrenamiento\\n \\nantes\\n \\nde\\n \\ngenerar\\n \\nuna\\n \\nrespuesta.\\n \\nLos\\n \\nmodelos\\n \\nde\\n \\nlenguaje\\n \\nde\\n \\ngran\\n \\ntamaño\\n \\n(LLM)\\n \\nse\\n \\nentrenan\\n \\ncon\\n \\nvolúmenes\\n \\nde\\n \\ndatos\\n \\namplios\\n \\ny\\n \\nusan\\n \\nmiles\\n \\nde\\n \\nmillones\\n \\nde\\n \\nparámetros\\n \\npara\\n \\ngenerar\\n \\nresultados\\n \\noriginales\\n \\nen\\n \\ntareas\\n \\ncomo\\n \\nresponder\\n \\npreguntas,\\n \\ntraducir\\n \\nidiomas\\n \\ny\\n \\ncompletar\\n \\nfrases.\\n \\nLa\\n \\nRAG\\n \\nextiende\\n \\nlas\\n \\nya\\n \\npoderosas\\n \\ncapacidades\\n \\nde\\n \\nlos\\n \\nLLM\\n \\na\\n \\ndominios\\n \\nespecíficos\\n \\no\\n \\na\\n \\nla\\n \\nbase\\n \\nde\\n \\nconocimientos\\n \\ninterna\\n \\nde\\n \\nuna\\n \\norganización,\\n \\ntodo\\n \\nello\\n \\nsin\\n \\nla\\n \\nnecesidad\\n \\nde\\n \\nvolver\\n \\na\\n \\nentrenar\\n \\nel\\n \\nmodelo.\\n \\nSe\\n \\ntrata\\n \\nde\\n \\nun\\n \\nmétodo\\n \\nrentable\\n \\npara\\n \\nmejorar\\n \\nlos\\n \\nresultados\\n \\nde\\n \\nlos\\n \\nLLM\\n \\nde\\n \\nmodo\\n \\nque\\n \\nsigan\\n \\nsiendo\\n \\nrelevantes,\\n \\nprecisos\\n \\ny\\n \\nútiles\\n \\nen\\n \\ndiversos\\n \\ncontextos.\\n   ¿Cuáles  son  los  beneficios  de  la  generación  aumentada  por  recuperación?   La  tecnología  RAG  aporta  varios  beneficios  a  los  esfuerzos  de  la  IA  generativa  de  una  \\norganización.\\n  ●  Implementación  rentable:  El  desarrollo  de  chatbots  normalmente  comienza  con  un  \\nmodelo\\n \\nfundacional.\\n \\nLos\\n \\nmodelos\\n \\nfundacionales\\n \\n(FM)\\n \\nson\\n \\nLLM\\n \\naccesibles\\n \\npara\\n \\nAPI\\n \\nentrenados\\n \\nen\\n \\nun\\n \\namplio\\n \\nespectro\\n \\nde\\n \\ndatos\\n \\ngeneralizados\\n \\ny\\n \\nsin\\n \\netiquetar.\\n \\nLos\\n \\ncostos\\n \\ncomputacionales\\n \\ny\\n \\nfinancieros\\n \\nde\\n \\nvolver\\n \\na\\n \\nentrenar\\n \\na\\n \\nlos\\n \\nFM\\n \\npara\\n \\nobtener\\n \\ninformación\\n \\nespecífica\\n \\nde\\n \\nla\\n \\norganización\\n \\no\\n \\ndel\\n \\ndominio\\n \\nson\\n \\naltos.\\n \\nLa\\n \\nRAG\\n \\nes\\n \\nun\\n \\nenfoque\\n \\nmás\\n \\nrentable\\n \\npara\\n \\nintroducir\\n \\nnuevos\\n \\ndatos\\n \\nen\\n \\nel\\n \\nLLM.\\n \\nHace\\n \\nque\\n \\nla\\n \\ntecnología\\n \\nde\\n \\ninteligencia\\n \\nartificial\\n \\ngenerativa\\n \\n(IA\\n \\ngenerativa)\\n \\nsea\\n \\nmás\\n \\naccesible\\n \\ny\\n \\nutilizable.\\n  ●  Información  actual:  Incluso  si  los  orígenes  de  datos  de  entrenamiento  originales  para  \\nun\\n \\nLLM\\n \\nson\\n \\nadecuados\\n \\npara\\n \\nsus\\n \\nnecesidades,\\n \\nes\\n \\ndifícil\\n \\nmantener\\n \\nla\\n \\nrelevancia.\\n \\nLa\\n \\nRAG\\n \\nles\\n \\npermite\\n \\na\\n \\nlos\\n \\ndesarrolladores\\n \\nproporcionar\\n \\nlas\\n \\núltimas\\n \\ninvestigaciones,\\n \\nestadísticas\\n \\no\\n \\nnoticias\\n \\na\\n \\nlos\\n \\nmodelos\\n \\ngenerativos.\\n \\nPueden\\n \\nusar\\n \\nla\\n \\nRAG\\n \\npara\\n \\nconectar\\n \\nel\\n \\nLLM\\n \\nde\\n \\nmanera\\n \\ndirecta\\n \\na\\n \\nredes\\n \\nsociales\\n \\nen\\n \\nvivo,\\n \\nsitios\\n \\nde\\n \\nnoticias\\n \\nu\\n \\notras\\n \\nfuentes\\n \\nde\\n \\ninformación\\n \\nque\\n \\nse\\n \\nactualizan\\n \\ncon\\n \\nfrecuencia.\\n \\nEl\\n \\nLLM\\n \\npuede\\n \\nentonces\\n \\nproporcionar\\n \\nla\\n \\ninformación\\n \\nmás\\n \\nreciente\\n \\na\\n \\nlos\\n \\nusuarios.\\n  ●  Mayor  confianza  de  los  usuarios:  La  RAG  le  permite  al  LLM  presentar  información  \\nprecisa\\n \\ncon\\n \\nla\\n \\natribución\\n \\nde\\n \\nla\\n \\nfuente.\\n \\nLa\\n \\nsalida\\n \\npuede\\n \\nincluir\\n \\ncitas\\n \\no\\n \\nreferencias\\n \\na\\n \\nfuentes.\\n \\nLos\\n \\nusuarios\\n \\ntambién\\n \\npueden\\n \\nbuscar\\n \\nellos\\n \\nmismos\\n \\nlos\\n \\ndocumentos\\n \\nde\\n \\norigen\\n \\nsi\\n \\nnecesitan\\n \\nmás\\n \\naclaraciones\\n \\no\\n \\nmás\\n \\ndetalles.\\n \\nEsto\\n \\npuede\\n \\naumentar\\n \\nla\\n \\nconfianza\\n \\nen\\n \\nsu\\n \\nsolución\\n \\nde\\n \\nIA\\n \\ngenerativa.\\n●  Más  control  para  los  desarrolladores:  Con  la  RAG,  los  desarrolladores  pueden  \\nprobar\\n \\ny\\n \\nmejorar\\n \\nsus\\n \\naplicaciones\\n \\nde\\n \\nchat\\n \\nde\\n \\nmanera\\n \\nmás\\n \\neficiente.\\n \\nPueden\\n \\ncontrolar\\n \\ny\\n \\ncambiar\\n \\nlas\\n \\nfuentes\\n \\nde\\n \\ninformación\\n \\ndel\\n \\nLLM\\n \\npara\\n \\nadaptarse\\n \\na\\n \\nlos\\n \\nrequisitos\\n \\ncambiantes\\n \\no\\n \\nal\\n \\nuso\\n \\nmultifuncional.\\n \\nLos\\n \\ndesarrolladores\\n \\ntambién\\n \\npueden\\n \\nrestringir\\n \\nla\\n \\nrecuperación\\n \\nde\\n \\ninformación\\n \\nconfidencial\\n \\na\\n \\ndiferentes\\n \\nniveles\\n \\nde\\n \\nautorización\\n \\ny\\n \\ngarantizar\\n \\nque\\n \\nel\\n \\nLLM\\n \\ngenere\\n \\nlas\\n \\nrespuestas\\n \\nadecuadas.\\n \\nAdemás,\\n \\ntambién\\n \\npueden\\n \\nsolucionar\\n \\nproblemas\\n \\ny\\n \\nhacer\\n \\ncorrecciones\\n \\nsi\\n \\nel\\n \\nLLM\\n \\nhace\\n \\nreferencia\\n \\na\\n \\nfuentes\\n \\nde\\n \\ninformación\\n \\nincorrectas\\n \\npara\\n \\npreguntas\\n \\nespecíficas.\\n \\nLas\\n \\norganizaciones\\n \\npueden\\n \\nimplementar\\n \\nla\\n \\ntecnología\\n \\nde\\n \\nIA\\n \\ngenerativa\\n \\ncon\\n \\nmayor\\n \\nconfianza\\n \\npara\\n \\nuna\\n \\ngama\\n \\nmás\\n \\namplia\\n \\nde\\n \\naplicaciones.\\n  Implementación  con  Langhain   1.  Cargamos  el  documento  con  PyPDFLoader  y  lo  convertimos  en  un  iterable.  Luego  \\nrecorremos\\n \\nese\\n \\niterable\\n \\ny\\n \\nguardamos\\n \\ncada\\n \\npage\\n \\nen\\n \\nuna\\n \\nvariable.\\n \\n  from langchain_community.document_loaders  import PyPDFLoader  #  cargar  el  documento document  =  PyPDFLoader(\"LANGCHAIN.pdf\")   #  creamos  el  iterador loader  =  document.lazy_load()   text  =  \"\" #  recorremos  el  contenido  del  loader   for page  in loader:       text  +=  page.page_content  2.  Importamos  nuestro  separador  de  texto,  lo  implementamos  y  creamos  los  documentos  que  \\ncargaremos\\n \\nposteriormente\\n \\nen\\n \\nnuestra\\n \\nbase\\n \\nde\\n \\ndatos\\n \\nvectorial:\\n \\n  from langchain_text_splitters  import CharacterTextSplitter  #  creamos  nuestro  separador  de  texto text_splitter  =  CharacterTextSplitter(      chunk_size=2000,      chunk_overlap=100,      separator=\"\\\\n\" )  #  creamos  los  documentos  para  cargar  nuestra  base  de  datos  vectorial texts  =  text_splitter.create_documents([text])\\n3.  Importamos  OllamaEmbedding  desde  langchain_ollama  para  utilizar  el  modelo  de  \\nembedding\\n  from langchain_ollama  import OllamaEmbeddings  #  creamos  la  comunicacion  con  nuestro  modelo  de  embedding embedding  =  OllamaEmbeddings(      model=\"nomic-embed-text\" )     4.  Ahora,  importamos  Chroma  desde  langchain_chroma  y  creamos  nuestra  vector  store   from langchain_chroma  import Chroma   #  declaramos  la  base  de  datos  vectorial vector_store  =  Chroma(      collection_name=\"test\",      embedding_function=embedding,      persist_directory=\"./vectorstore-nomic\" )  5.  Cargamos  los  documentos  que  creamos  en  el  punto  2   #  creamos  los  embeddings  y  lo  guardamos  en  la  base  de  datos  vectorial   vector_store.add_documents(texts)  6.  En  este  punto,  ya  tenemos  documentos  en  nuestra  base  de  datos  vectorial,  por  lo  tanto,  \\nvamos\\n \\na\\n \\ncrear\\n \\nnuestro\\n \\nllm\\n \\nque\\n \\nresponderá\\n \\npreguntas\\n \\nutilizando\\n \\nla\\n \\ninformación\\n \\nque\\n \\nguardamos'), Document(id='33f8106e-dbf7-4d19-abda-0fd91dfb6e3c', metadata={}, page_content='1. Investiga y explica con tus palabras cada uno de los siguientes conceptos(podés usar libros, videos, artículos, papers o apuntes de clase):○ ¿Qué es una red neuronal artificial?Una red neuronal artificial es una red neuronal que es artificial porque no es real, es creada por nosotros, es decir es creada por el humano pero no de nacimiento sino que fue creado por nosotros (Son nodos que se encargan de realizar una predicción a partir de datos que se le brindan a través de tensores que pueden estar más o menos abstraidos dependiendo de la libreria que se utiliza)○ ¿Qué función cumplen las capas (entrada, ocultas, salida)?Las funciones que cumoplen las capas son:Entrada: Reciben los datos que le mandamos (ej: pixeles de una imagen, números, etc).Oculta: Se encargan de hacer los cálculos y las transformaciones inmediatas.Salida: Devuelve la predicción o un mensaje final de los datos recibidos, como por ej: “gato”, “spam”, “positivo”, etc.○ ¿Qué es un forward pass y por qué es importante?Un \"Forward Pass\" (también conocido como \"Forward Propagation\") es la fase en la que una red neuronal procesa una entrada y genera una salida. Se lo puede entender como el \"camino hacia adelante\" desde la capa de entrada hasta la capa de salida.Es importante porque es el primer paso fundamental en el ciclo de entrenamiento de una red neuronal, después de que la red realiza una predicción mediante el forward pass, esta salida se compara con el valor real para calcular el error o \"loss\"○ ¿Qué es una función de activación? Nombra al menos tres y explicaen qué se diferencian.Una función de activación es una función matemática que va a decidir si una neurona está activa o no, es decir, cuanta cantidad de la información recibida debe pasar a la siguiente capa. Sin una función de activación, la red neuronal no podría aprender relaciones complejas ni resolver problemas como clasificación lineal.Relu: Trabaja con datos de entrenamiento más complejo donde hay mucha informaciónSigmoid: Trabaja con numeros binarios dando una respuesta como 1 o 0Softmax: Se utiliza en la salida de una red neuronal y es multiclase haciendo que todos los resultados de analizar la información den como resultado 1.○ ¿Qué es una función de pérdida (loss function) y para qué seutiliza? El loss o función de pérdida mide que tan mal está prediciendo una red neuronal. Se encarga de decir a la red en cuanto se está equivocando.compara la salida predicha por la red (ŷ) con el valor real (y) y devuelve un numero:\\n. Si el número es grande, la predicción es mala.. Si el número es cero o pequeño, la predicción es buena.○ ¿Qué es la backpropagation? ¿Cómo se relaciona con el cálculo dederivadas?La Backpropagation es un algoritmo clave que le permite a una red neuronal aprender, funcionando como su sistema de corrección de errores, después de que la red hace una predicción y se calcula el error (o \"loss\"), la Backpropagation calcula cuánto contribuyó cada \"peso\" al error. Esto se relaciona con el cálculo de derivadas porque utiliza \"gradientes\" para saber la dirección y la intensidad en que deben ajustarse los pesos y sesgos.2. Elegí un optimizador (por ejemplo: Adam o SGD) e investiga:Elegimos el optimizador Adagrad ○ ¿Quién lo propuso? (Nombre y año si está disponible)Lo propusieron John Duchi, Elad Hazan y Yoram Singer en el año 2011\"Adaptive Subgradient Methods for Online Learning and Stochastic Optimization\"○ ¿En qué tipo de problemas suele usarse?Se utiliza principalmente en problemas de aprendizaje automático donde los datos son dispersos, como en el procesamiento de lenguaje natural (NLP), la clasificación de texto, la recomendación de sistemas y el aprendizaje en redes neuronales con características de alta dimensionalidad. Su principal ventaja es que adapta automáticamente la tasa de aprendizaje para cada parámetro, asignando tasas más pequeñas a parámetros con gradientes frecuentes y mayores a aquellos con gradientes menos frecuentes, lo que lo hace especialmente efectivo en conjuntos de datos con características esparsas o cuando los gradientes varían significativamente en magnitud.○ ¿Cuáles son sus ventajas y desventajas?Como ventajas:● tiene una tasa de aprendizaje adaptativa que significa que los parametros que cambian mucho se actualizan menos y los que cambian poco se actualizan más●Buenardo para datos dispersos como por ejemplo para modelos de lenguaje naturalComo desventajas: ●No es recomendable usarlo para modelos grandes o para entrenamientos largos●Una vez que deja de aprender se suele quedar atascado3. Reflexión crítica:¿Por qué pensás que el proceso de entrenamiento en redes neuronalesnecesita tantas etapas?Creo que necesita varias etapas porque hay que asegurarse de que los datos de entrenamiento pasen por un camino más limpio para obtener una salida esperada, ya sea limpiando los set de datos, ajustando tamaños de datos de entrenamiento como los de testing, también verificar que los datos de perdida sean los adecuados y evitar anomalias en el entrenamiento de la red neuronal.\\n¿Qué pasaría si no existiera el paso deretropropagación?Quizás cuando el entrenamiento no salga como esperamos los datos de salida del modelo estén más sesgados por datos que no son beneficiosos, por lo tanto el modelo respondería pero de forma erronea.También el aprendizaje sería muy lento o quizás inutil porque se tendria que usar métodos básicos como prueba y error, lo que haría que el entrenamiento sea muy impreciso o directamente inviable en redes grandes.La razón por la que debe de necesitar tantas etapas es porque aprende de apoco y en cada etapa la red ajusta los pesos y sesgos para acercarse a la solución correcta. Esto se repite muchas veces (épocas) porque los datos al ser muchos o complejos se necesitan verlos varias veces para reconocer patrones, minimizar errores , entre otros.'), Document(id='a7316757-0881-4681-994e-7053370fcfe4', metadata={}, page_content='nuestro\\n \\nllm\\n \\nque\\n \\nresponderá\\n \\npreguntas\\n \\nutilizando\\n \\nla\\n \\ninformación\\n \\nque\\n \\nguardamos\\nfrom langchain_google_genai  import ChatGoogleGenerativeAI  from dotenv  import load_dotenv  import os   load_dotenv()   api_key  =  os.getenv(\"API_KEY\")   llm  =  ChatGoogleGenerativeAI(      api_key=api_key,      model  =  \"gemini-2.5-flash\",      temperature=0.5 )  7.  Creamos  un  prompt_template  para  nuestro  llm  con  ChatPromptTemplate   from langchain_core.prompts  import ChatPromptTemplate  ai_msg  =  [\"\"]  human_msg  =  []   prompt_template  =  ChatPromptTemplate.from_messages([      (\"system\",\"\"\"      Eres  un  asistente  encargado  de  responder  preguntas  sobre  Langchain.                     Responde  solo  si   {context}  posee  contenido.  Si  el  contexto  esta  vacio,                      responde  \"No  tengo  suficiente  informacion  para  reponder  esa  pregunta\"      \"\"\"),      (\"ai\",\"{ai_msg}\"),      (\"human\",\"{human_msg}\")  ])\\n8.  Casi  terminamos,  ahora  vamos  a  declarar  una  variable  donde  utilizaremos  un  input,  para  \\nprobar\\n \\nel\\n \\nfuncionamiento\\n \\nde\\n \\nnuestro\\n \\nrag.\\n  #  solicitud  del  usuario input_user  =  input(\"Human:  \")  human_msg.append(input_user)  9.  La  solicitud  del  punto  8,  utilizaremos  para  hacer  una  búsqueda  vectorial  con  \\nsimilitary_search.\\n \\nLuego,\\n \\npasaremos\\n \\nun\\n \\ndiccionario\\n \\na\\n \\nnuestro\\n \\nprompt_template\\n \\npara\\n \\nrellenar\\n \\nlas\\n \\nvariables\\n \\nque\\n \\nestá\\n \\nesperando.\\n \\n  docs  =  vector_store.similarity_search(input_user,  k=10)   prompt  =  prompt_template.invoke(  {      \"context\":  docs,      \"ai_msg\":  ai_msg,      \"human_msg\":  human_msg  })  10.  Invocamos  el  modelo  con  el  método  invoke   response  =  llm.invoke(prompt)   print(response.content)    Resultado  obtenido:   ╭─  ~\\\\Desktop\\\\langchain  ╰─   Human:  que  son  los  prompt  templates?    AI:  Los  Prompt  Templates  en  Langchain  son  clases  que  permiten  crear  plantillas  de  mensajes  reutilizables.  Estas  plantillas  pueden  definir  variables  (encerradas  entre  llaves  `{}`)  que  se  llenarán  con  valores  específicos  al  momento  de  invocar  el  modelo.  Existe  también  la  clase  `ChatPromptTemplate`,  que  a  diferencia  de  `PromptTemplate`,  espera  un  diccionario  con  las  variables  declaradas  en  el  template.\\nActividades   Todo  el  contenido  de  la  actividad,  debe  ser  entregado  en  un  júpiter  notebook,  con  sus  \\ncorrespondientes\\n \\nceldas\\n \\nde\\n \\nmarkdown\\n \\npara\\n \\nla\\n \\ndocumentación\\n \\ny\\n \\nceldas\\n \\nde\\n \\ncódigo.\\n \\n1.  Implementación  Práctica  de  RAG:  \\n●  Recrea  el  ejemplo  dado  en  un  Jupyter  Notebook  o  entorno  de  desarrollo  similar,  \\nutilizando\\n \\nlas\\n \\nlibrerías\\n \\nlangchain_community.document_loaders,\\n \\nlangchain_text_splitters,\\n \\nlangchain_ollama,\\n \\nlangchain_chroma,\\n \\ny\\n \\nlangchain_google_genai.\\n ●  Verifica  el  funcionamiento  del  sistema  RAG  respondiendo  a  preguntas  relacionadas  \\ncon\\n \\nel\\n \\ncontenido\\n \\nde\\n \\nun\\n \\ndocumento\\n \\nX\\n \\n(deben\\n \\nproporcionar\\n \\nuno\\n \\nustedes).\\n \\n2.  Exploración  de  Modelos  de  Embedding  y  LLM:  \\n●  Experimenta  con  diferentes  modelos  de  embedding  disponibles  en  langchain_ollama  \\n(además\\n \\nde\\n \\n\"nomic-embed-text\")\\n \\ny\\n \\nanaliza\\n \\ncómo\\n \\nimpactan\\n \\nen\\n \\nla\\n \\ncalidad\\n \\nde\\n \\nla\\n \\nrecuperación\\n \\nde\\n \\ndocumentos.\\n ●  Prueba  con  otros  modelos  de  lenguaje  de  gran  tamaño  (LLM)  compatibles  con  \\nlangchain_google_genai\\n \\no\\n \\nlangchain_ollama\\n \\ny\\n \\ncompara\\n \\nsus\\n \\nrespuestas\\n \\ny\\n \\nrendimiento.\\n \\n3.  Optimización  del  Separador  de  Texto:  \\n●  Modifica  los  parámetros  chunk_size  y  chunk_overlap  del  CharacterTextSplitter  y  \\nobserva\\n \\ncómo\\n \\nafectan\\n \\na\\n \\nla\\n \\ncreación\\n \\nde\\n \\nlos\\n \\ndocumentos\\n \\ny,\\n \\nconsecuentemente,\\n \\na\\n \\nla\\n \\nprecisión\\n \\nde\\n \\nlas\\n \\nrespuestas\\n \\ndel\\n \\nLLM.\\n ●  Investiga  otros  tipos  de  separadores  de  texto  disponibles  en  Langchain  y  evalúa  su  \\nidoneidad\\n \\npara\\n \\ndiferentes\\n \\ntipos\\n \\nde\\n \\ndocumentos.\\n \\n4.  Gestión  de  Bases  de  Datos  Vectoriales:  \\n●  Explora  las  funcionalidades  de  Chroma  para  gestionar  colecciones,  persistencia  de  \\ndatos\\n \\ny\\n \\nrealizar\\n \\nbúsquedas\\n \\nmás\\n \\navanzadas\\n \\n(por\\n \\nejemplo,\\n \\nfiltrado\\n \\nde\\n \\nresultados).\\n ●  Investiga  otras  bases  de  datos  vectoriales  compatibles  con  Langchain  y  considera  sus  \\nventajas\\n \\ny\\n \\ndesventajas\\n \\npara\\n \\ndiferentes\\n \\ncasos\\n \\nde\\n \\nuso.\\n \\n5.  Refinamiento  de  Prompts:  \\n●  Experimenta  con  diferentes  prompt_template  para  el  LLM,  ajustando  las  instrucciones  \\ndel\\n \\nsistema\\n \\ny\\n \\nlos\\n \\nmensajes\\n \\nde\\n \\nIA/humano\\n \\npara\\n \\nmejorar\\n \\nla\\n \\ncalidad\\n \\ny\\n \\nrelevancia\\n \\nde\\n \\nlas\\n \\nrespuestas.\\n ●  Implementa  técnicas  de  ingeniería  de  prompts  para  guiar  al  LLM  a  proporcionar  \\nrespuestas\\n \\nmás\\n \\nprecisas\\n \\ny\\n \\nútiles,\\n \\nincluyendo\\n \\nla\\n \\natribución\\n \\nde\\n \\nfuentes\\n \\ncuando\\n \\nsea\\n \\nposible.')]\n",
      "En el contexto de una red neuronal artificial, los nodos son los elementos que se encargan de realizar una predicción a partir de datos que se les brindan a través de tensores. La abstracción de estos tensores puede variar dependiendo  de la librería utilizada.  "
     ]
    }
   ],
   "source": [
    "\n",
    "for l in range(2):\n",
    "    input_user = input(\"Human: \")\n",
    "    print(input_user)\n",
    "\n",
    "    docs = retrieval(input_user=input_user)\n",
    "    print(docs)\n",
    "\n",
    "    for chunk in response(input_user=input_user, contexto=docs):\n",
    "        print(chunk, end=\" \", flush=True)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
